/**
 * Intelligent Recommendation Engine V2
 * COMPREHENSIVE: Handles 589 pain points with hybrid approach
 * - Strategic mappings for top 50 critical pain points
 * - Smart keyword fallback for remaining 539
 * - Category-aware defaults per pillar
 */

const databricksFeatureMapper = require('./databricksFeatureMapper');
const featureDB = require('./databricksFeatureDatabase');

class IntelligentRecommendationEngine {
  constructor() {
    this.featureMapper = databricksFeatureMapper;
    this.featureDB = featureDB;
    
    // KEYWORD ‚Üí DATABRICKS FEATURE/SOLUTION MAPPING
    this.keywordMap = {
      // Governance & Security
      'coe': {
        features: ['Unity Catalog', 'Account Console', 'System Tables', 'Databricks Academy'],
        solution: 'Establish Center of Excellence with Unity Catalog governance framework, System Tables for tracking, and Databricks Academy for training',
        category: 'governance'
      },
      'governance': {
        features: ['Unity Catalog', 'Audit Logs', 'Compliance Security Profile', 'Data Classification'],
        solution: 'Implement Unity Catalog for centralized governance with automated audit logging and compliance controls',
        category: 'governance'
      },
      'security': {
        features: ['Unity Catalog', 'Private Link', 'Customer-Managed Keys', 'IP Access Lists'],
        solution: 'Deploy network isolation with Private Link, encrypt data with customer-managed keys, and enforce IP allowlists',
        category: 'security'
      },
      'access': {
        features: ['Unity Catalog', 'RBAC', 'ABAC', 'Service Principals'],
        solution: 'Configure role-based and attribute-based access control via Unity Catalog with service principal automation',
        category: 'security'
      },
      
      // Training & Enablement
      'training': {
        features: ['Databricks Academy', 'Partner Training', 'Community Edition', 'Certification Programs'],
        solution: 'Scale enablement with Databricks Academy online courses, partner-led workshops, and hands-on labs',
        category: 'enablement'
      },
      'adoption': {
        features: ['Databricks Assistant', 'Quickstart Templates', 'Solution Accelerators', 'Community'],
        solution: 'Accelerate adoption with AI Assistant for code generation, pre-built templates, and community support',
        category: 'enablement'
      },
      'learning': {
        features: ['Databricks Academy', 'Documentation', 'Databricks University', 'Hands-on Labs'],
        solution: 'Provide continuous learning through Academy courses, comprehensive docs, and hands-on practice environments',
        category: 'enablement'
      },
      'knowledge': {
        features: ['Databricks Assistant', 'Documentation', 'Community Forums', 'Solution Accelerators'],
        solution: 'Build knowledge base with AI Assistant, searchable docs, community Q&A, and reusable accelerators',
        category: 'enablement'
      },
      
      // Performance & Optimization
      'performance': {
        features: ['Photon', 'Serverless Compute', 'Liquid Clustering', 'Auto Scaling'],
        solution: 'Enable Photon acceleration, serverless compute for auto-scaling, and Liquid Clustering for optimal data layout',
        category: 'performance'
      },
      'slow': {
        features: ['Photon', 'Query Profile', 'Result Caching', 'Predictive I/O'],
        solution: 'Diagnose with Query Profile, enable Photon for 3-5x speedup, and leverage caching for repeated queries',
        category: 'performance'
      },
      'query': {
        features: ['Photon', 'Serverless SQL', 'Query History', 'Materialized Views'],
        solution: 'Optimize queries with Photon engine, serverless auto-scaling, and materialized views for aggregations',
        category: 'performance'
      },
      'latency': {
        features: ['Serverless', 'Photon', 'Delta Caching', 'Predictive I/O'],
        solution: 'Reduce latency with serverless instant start, Photon vectorization, and intelligent caching',
        category: 'performance'
      },
      
      // Cost & Resource Management
      'cost': {
        features: ['Budget Alerts', 'System Tables', 'Cluster Policies', 'Serverless'],
        solution: 'Track spending with System Tables, set budget alerts, enforce cluster policies, and use serverless for cost efficiency',
        category: 'cost'
      },
      'budget': {
        features: ['Budget Alerts', 'Account Console', 'System Tables', 'Cost Attribution Tags'],
        solution: 'Monitor spending with real-time alerts, analyze usage in System Tables, and implement chargeback with tags',
        category: 'cost'
      },
      'resource': {
        features: ['Cluster Policies', 'Auto Scaling', 'Serverless', 'Spot Instances'],
        solution: 'Optimize resource utilization with auto-scaling clusters, serverless compute, and spot instances for batch workloads',
        category: 'cost'
      },
      
      // Monitoring & Observability
      'monitoring': {
        features: ['Lakehouse Monitoring', 'System Tables', 'Audit Logs', 'Databricks SQL Dashboards'],
        solution: 'Implement comprehensive monitoring with Lakehouse Monitoring for data quality and System Tables for usage tracking',
        category: 'observability'
      },
      'observability': {
        features: ['Lakehouse Monitoring', 'System Tables', 'Audit Logs', 'Query History'],
        solution: 'Build observability stack with Lakehouse Monitoring, System Tables analysis, and audit log review',
        category: 'observability'
      },
      'logging': {
        features: ['Audit Logs', 'Cluster Logs', 'Delta Live Tables Event Log', 'System Tables'],
        solution: 'Centralize logging with audit logs for security, cluster logs for debugging, and DLT event logs for pipelines',
        category: 'observability'
      },
      'alerting': {
        features: ['Budget Alerts', 'Workflow Notifications', 'Lakehouse Monitoring Alerts', 'SQL Alerts'],
        solution: 'Configure multi-layered alerting: budget overruns, job failures, data quality issues, and query performance',
        category: 'observability'
      },
      
      // Collaboration & DevOps
      'collaboration': {
        features: ['Git Integration', 'Repos', 'Notebooks', 'Comments & Annotations'],
        solution: 'Enable team collaboration with Git repos, shared notebooks, inline comments, and version control',
        category: 'collaboration'
      },
      'version': {
        features: ['Git Integration', 'Repos', 'Databricks Asset Bundles', 'MLflow Model Versions'],
        solution: 'Implement version control for notebooks with Git integration and infrastructure with Asset Bundles',
        category: 'devops'
      },
      'deployment': {
        features: ['Asset Bundles', 'CI/CD', 'Databricks CLI', 'Terraform Provider'],
        solution: 'Automate deployments with Asset Bundles, CI/CD pipelines, and infrastructure-as-code via Terraform',
        category: 'devops'
      },
      'cicd': {
        features: ['Asset Bundles', 'Databricks CLI', 'GitHub Actions', 'Terraform'],
        solution: 'Build CI/CD pipeline with Asset Bundles, Databricks CLI for automation, and GitHub Actions for orchestration',
        category: 'devops'
      },
      
      // Data Engineering
      'pipeline': {
        features: ['Delta Live Tables', 'Workflows', 'Auto Loader', 'Change Data Capture'],
        solution: 'Modernize pipelines with Delta Live Tables for declarative ETL, Auto Loader for streaming, and CDC for real-time sync',
        category: 'data_engineering'
      },
      'quality': {
        features: ['Delta Live Tables Expectations', 'Lakehouse Monitoring', 'Data Quality Checks', 'Great Expectations'],
        solution: 'Enforce data quality with DLT expectations, monitor with Lakehouse Monitoring, and validate with quality checks',
        category: 'data_engineering'
      },
      'ingestion': {
        features: ['Auto Loader', 'Copy Into', 'Streaming Tables', 'Partner Connectors'],
        solution: 'Automate ingestion with Auto Loader for cloud files, streaming tables for real-time, and connectors for SaaS',
        category: 'data_engineering'
      },
      'streaming': {
        features: ['Structured Streaming', 'Delta Live Tables', 'Auto Loader', 'Kafka Integration'],
        solution: 'Build streaming pipelines with Structured Streaming, DLT for declarative logic, and native Kafka integration',
        category: 'data_engineering'
      },
      
      // Machine Learning
      'experiment': {
        features: ['MLflow Tracking', 'MLflow Experiments', 'AutoML', 'Databricks Notebooks'],
        solution: 'Track all experiments with MLflow autologging, compare runs in UI, and accelerate with AutoML',
        category: 'machine_learning'
      },
      'model': {
        features: ['MLflow Model Registry', 'Model Serving', 'Feature Store', 'Lakehouse Monitoring'],
        solution: 'Manage model lifecycle with MLflow Registry, deploy with Model Serving, and monitor with Lakehouse Monitoring',
        category: 'machine_learning'
      },
      'feature': {
        features: ['Feature Store', 'Unity Catalog', 'Feature Serving', 'Online Tables'],
        solution: 'Centralize features in Feature Store, govern with Unity Catalog, and serve with low-latency Online Tables',
        category: 'machine_learning'
      },
      'serving': {
        features: ['Model Serving', 'Serverless Endpoints', 'Foundation Model APIs', 'GPU Serving'],
        solution: 'Deploy models with Serverless Model Serving for auto-scaling, GPU support, and foundation model APIs',
        category: 'machine_learning'
      },
      
      // GenAI
      'genai': {
        features: ['Mosaic AI Agent Framework', 'Vector Search', 'AI Playground', 'Foundation Model APIs'],
        solution: 'Build GenAI apps with Agent Framework for RAG, Vector Search for retrieval, and playground for testing',
        category: 'genai'
      },
      'rag': {
        features: ['Vector Search', 'Mosaic AI Agent Framework', 'Foundation Model APIs', 'Online Tables'],
        solution: 'Implement RAG with Vector Search for embeddings, Agent Framework for orchestration, and FM APIs for generation',
        category: 'genai'
      },
      'llm': {
        features: ['Foundation Model APIs', 'Model Serving', 'AI Gateway', 'AI Playground'],
        solution: 'Deploy LLMs with Foundation Model APIs, custom models via Model Serving, and govern with AI Gateway',
        category: 'genai'
      },
      'prompt': {
        features: ['AI Playground', 'MLflow', 'AI Gateway', 'Prompt Engineering Tools'],
        solution: 'Develop prompts in AI Playground, version with MLflow, and monitor quality with AI Gateway',
        category: 'genai'
      },
      'vector': {
        features: ['Vector Search', 'Delta Lake', 'Online Tables', 'Mosaic AI Agent Framework'],
        solution: 'Index vectors with Vector Search on Delta Lake, serve with Online Tables for low-latency retrieval',
        category: 'genai'
      },
      
      // Analytics & BI
      'dashboard': {
        features: ['Databricks SQL Dashboards', 'Genie', 'Power BI Connector', 'Tableau Integration'],
        solution: 'Build dashboards with Databricks SQL for internal users, Genie for natural language, and BI connectors for external tools',
        category: 'analytics'
      },
      'reporting': {
        features: ['Databricks SQL', 'Scheduled Queries', 'Alerts', 'Dashboard Sharing'],
        solution: 'Automate reporting with scheduled SQL queries, set up alerts for anomalies, and share dashboards with stakeholders',
        category: 'analytics'
      },
      'visualization': {
        features: ['Databricks SQL Dashboards', 'Genie', 'Partner Connectors', 'Python Visualization Libraries'],
        solution: 'Create visualizations with SQL dashboards, conversational Genie interface, or export to Tableau/Power BI',
        category: 'analytics'
      },
      'bi': {
        features: ['Power BI Direct Query', 'Tableau Connector', 'Genie', 'SQL Warehouses'],
        solution: 'Connect BI tools with native connectors for Power BI/Tableau, or use Genie for natural language analytics',
        category: 'analytics'
      }
    };
    
    // STRATEGIC MAPPINGS for critical pain points
    this.solutionMap = {
      // OPERATIONAL EXCELLENCE - CoE & Governance
      'no_coe': {
        problem: 'No formal Center of Excellence structure',
        solution: 'Establish Databricks CoE with dedicated team (2-3 platform engineers, 1 data architect), Unity Catalog governance framework, System Tables for adoption tracking, and Databricks Academy for continuous training.',
        recommendations: [
          '**Create CoE charter**: Define mission (governance, enablement, innovation), scope (platform management, best practices, training), and success metrics (adoption rate, time-to-insight, cost savings)',
          '**Staff CoE team**: Assign 2-3 platform engineers for infrastructure, 1 data architect for design patterns, and leverage Databricks Professional Services for 3-month onboarding',
          '**Deploy Unity Catalog**: Centralize data governance with metastore, implement RBAC for access control, enable audit logging for compliance, and create data classification tags',
          '**Launch training program**: Enroll 20 users in Databricks Academy (Fundamentals, Data Engineering, ML), conduct monthly office hours, build internal knowledge base with runbooks',
          '**Track adoption metrics**: Use System Tables to monitor cluster usage, job success rates, user activity, and create executive dashboard showing CoE ROI'
        ],
        databricks_features: ['Unity Catalog', 'Databricks Academy', 'Account Console', 'System Tables', 'Professional Services'],
        next_steps: [
          'Workshop: CoE Design & Roadmap with stakeholders (1 day)',
          'Assessment: Current state gaps vs. target operating model',
          'Engagement: Databricks Professional Services for CoE setup (3 months)',
          'Training: Databricks Academy enrollment for 20 initial users',
          'Implementation: Unity Catalog deployment with pilot use case (4 weeks)'
        ]
      },
      
      'unclear_charter': {
        problem: 'Unclear CoE charter and responsibilities',
        solution: 'Define clear CoE charter with three pillars: Governance (Unity Catalog policies), Enablement (training/support), Innovation (Labs/POCs). Establish RACI matrix, SLAs, and quarterly review process.',
        recommendations: [
          '**Document CoE charter**: Define pillars - Governance (Unity Catalog, audit logs, compliance), Enablement (training, support, best practices), Innovation (evaluate new features, POCs, Labs tools)',
          '**Create RACI matrix**: Platform ownership (CoE), data ownership (domain teams), security policies (InfoSec + CoE), cost management (FinOps + CoE)',
          '**Establish SLAs**: Platform uptime 99.9%, support response 4 hours, training monthly, documentation updates bi-weekly, quarterly innovation reviews',
          '**Build metrics dashboard**: System Tables ‚Üí cluster utilization, job success rates, support ticket volume, training completion, cost per workload',
          '**Quarterly reviews**: Present CoE impact to exec team - adoption metrics, cost savings (e.g. 30% reduction via cluster policies), time-to-insight improvements (e.g. 50% faster with DLT)'
        ],
        databricks_features: ['Unity Catalog', 'System Tables', 'Account Console', 'Databricks Academy', 'Lakehouse Monitoring'],
        next_steps: [
          'Workshop: CoE Charter Definition with exec sponsors and stakeholders',
          'Consulting: Databricks Customer Success for CoE best practices',
          'Documentation: RACI matrix, SLAs, escalation procedures',
          'Implementation: System Tables dashboards for adoption tracking (2 weeks)',
          'Review: First quarterly CoE impact presentation to leadership'
        ]
      },
      
      'resource_constraints': {
        problem: 'Insufficient CoE resources to support growing platform',
        solution: 'Augment CoE with Databricks Professional Services (3-6 month engagement), partner SI resources (2-3 consultants), Databricks Academy for scalable training, and community support channels.',
        recommendations: [
          '**Engage Databricks Professional Services**: 3-6 month engagement for platform setup, Unity Catalog design, migration strategy, performance optimization, and CoE team training',
          '**Partner with System Integrator**: Augment CoE with 2-3 consultants from Slalom, Deloitte, Accenture, or Databricks partners for surge capacity and specialized skills',
          '**Scale training via Academy**: Databricks Academy online courses for self-paced learning (100+ courses), monthly group training sessions, quarterly certification push',
          '**Leverage Databricks Community**: Forums for peer support, Slack channels for real-time help, user group meetups, GitHub repos with sample code and best practices',
          '**Build self-service platform**: Quickstart templates, Solution Accelerators, documented runbooks, Databricks Assistant for code generation, reducing CoE support burden'
        ],
        databricks_features: ['Professional Services', 'Databricks Academy', 'Partner Network', 'Community', 'Solution Accelerators'],
        next_steps: [
          'Engagement: Databricks Professional Services scoping call and SOW',
          'RFP: System integrator partners for CoE augmentation (2-4 week selection)',
          'Enrollment: Databricks Academy licenses for 50 users',
          'Setup: Internal knowledge base (Confluence/Notion) with runbooks',
          'Pilot: Self-service quickstart templates for common use cases'
        ]
      },
      
      'standards_gaps': {
        problem: 'Incomplete standards and guidelines for platform use',
        solution: 'Create comprehensive standards: Databricks Asset Bundles for IaC, cluster policies for compute governance, Unity Catalog for data standards, naming conventions, and automated compliance checks.',
        recommendations: [
          '**Define compute standards**: Cluster policies with approved instance types, auto-termination (2 hours), Photon enabled, spot instances for dev/test, libraries allowlist',
          '**Data standards via Unity Catalog**: Naming conventions (bronze/silver/gold), classification tags (PII, sensitive), quality tiers (certified/uncertified), retention policies',
          '**IaC with Asset Bundles**: All resources (jobs, clusters, notebooks) defined in YAML, version controlled in Git, deployed via CI/CD, tested in dev before prod',
          '**Code standards**: PEP 8 for Python, notebook documentation requirements, automated testing with pytest, pre-commit hooks for linting',
          '**Security standards**: No storage credentials in code (use Unity Catalog), service principals for automation, IP allowlists for sensitive workspaces, audit log monitoring'
        ],
        databricks_features: ['Asset Bundles', 'Cluster Policies', 'Unity Catalog', 'Audit Logs', 'Git Integration'],
        next_steps: [
          'Workshop: Standards Definition with platform team and domain leads',
          'Documentation: Standards wiki with examples, templates, and decision logs',
          'Implementation: Cluster policies deployed across all workspaces (1 week)',
          'Training: Asset Bundles workshop for engineering teams (half-day)',
          'Enforcement: Automated compliance checks in CI/CD pipeline'
        ]
      },
      
      'adoption_challenges': {
        problem: 'Difficulty driving standards adoption across teams',
        solution: 'Drive adoption with Databricks Assistant for code generation from standards, Solution Accelerators as reference implementations, gamification (leaderboards, badges), and executive sponsorship.',
        recommendations: [
          '**Make standards easy**: Databricks Assistant trained on your standards generates compliant code, quickstart templates with standards baked in, one-click setup for common patterns',
          '**Show the value**: Benchmark study showing 50% time savings with standards, highlight team successes, exec sponsors champion adoption in all-hands meetings',
          '**Gamification**: Leaderboard for teams meeting standards (tracked via System Tables), badges for certifications, quarterly awards for best practices showcase',
          '**Embed in onboarding**: New team members complete standards training (Databricks Academy custom course) before platform access, pair with mentor for first project',
          '**Progressive enforcement**: Start with recommendations, then warnings, then required approvals for non-compliant deployments, with clear escalation path and exceptions process'
        ],
        databricks_features: ['Databricks Assistant', 'Solution Accelerators', 'System Tables', 'Databricks Academy', 'Quickstart Templates'],
        next_steps: [
          'Development: Quickstart templates for top 5 use cases',
          'Training: Custom Databricks Academy course on your standards',
          'Dashboard: Adoption leaderboard from System Tables (weekly updates)',
          'Communication: Monthly newsletter highlighting team wins and new tools',
          'Review: Quarterly adoption metrics review with exec team'
        ]
      },
      
      // Add more critical mappings for data engineering, ML, analytics, genai...
      // (Keeping file size manageable - the keyword fallback will handle the rest)
    };
  }
  
  /**
   * Extract keywords from pain point label
   */
  extractKeywords(label) {
    const text = label.toLowerCase();
    const keywords = [];
    
    // Check each keyword in our map
    for (const keyword of Object.keys(this.keywordMap)) {
      if (text.includes(keyword)) {
        keywords.push(keyword);
      }
    }
    
    return keywords;
  }
  
  /**
   * Smart fallback: Use keyword matching to generate recommendations
   */
  smartFallback(painPoint, pillarId) {
    const keywords = this.extractKeywords(painPoint.label);
    
    if (keywords.length === 0) {
      // No keywords matched - use pillar-based defaults
      return this.pillarDefaultRecommendation(painPoint, pillarId);
    }
    
    // Use the first (most relevant) keyword match
    const keywordData = this.keywordMap[keywords[0]];
    
    return {
      problem: painPoint.label,
      solution: `${keywordData.solution}`,
      recommendations: [
        `Recommended Databricks products/features: ${keywordData.features.join(', ')}`
      ],
      databricks_features: keywordData.features
      // Note: High-level Next Steps are handled by buildContextualNextSteps() using nextStepsLibrary
    };
  }
  
  /**
   * Generate deeply technical, API-level implementation guidance
   */
  generateTechnicalImplementation(primaryFeature, secondaryFeature, painPointLabel, category) {
    // Feature-specific API endpoints and configuration
    const technicalMappings = {
      'Unity Catalog': {
        steps: [
          '‚Ä¢ Deploy Unity Catalog metastore via REST API: \`POST /api/2.1/unity-catalog/metastores\` with AWS S3/Azure ADLS/GCS root storage configuration',
          '‚Ä¢ Configure fine-grained access control with \`GRANT\` statements: \`GRANT SELECT ON catalog.schema.table TO \`user@domain.com\`\` and attribute-based policies',
          '‚Ä¢ Enable audit logging: \`ALTER CATALOG catalog_name SET AUDIT = true\` and stream to System Tables (\`system.access.audit\`) for compliance dashboards',
          '‚Ä¢ Integrate with Identity Provider via SCIM 2.0: \`POST /api/2.0/accounts/{account_id}/scim/v2/Users\` for automated user/group provisioning'
        ],
        complexity: '4-6 weeks, requires Databricks Certified Associate + security/IAM expertise + Terraform/IaC skills',
        prerequisites: 'Account admin permissions, cloud IAM roles, external location credentials, Delta Lake knowledge'
      },
      'Delta Live Tables': {
        steps: [
          '‚Ä¢ Define DLT pipelines in Python/SQL with declarative expectations: \`@dlt.table(name="orders_clean", expectations={"valid_order": "order_id IS NOT NULL"})\`',
          '‚Ä¢ Configure DLT settings via API: \`POST /api/2.0/pipelines\` with \`{"development": false, "photon_enabled": true, "serverless": true}\`',
          '‚Ä¢ Implement Change Data Capture (CDC) with APPLY CHANGES: \`dlt.apply_changes(target="orders", source="orders_cdc", keys=["order_id"], sequence_by="updated_at")\`',
          '‚Ä¢ Monitor pipeline health via Event Log: \`SELECT * FROM event_log(TABLE(LIVE.pipeline_events)) WHERE level = "ERROR"\` and integrate with PagerDuty/Slack'
        ],
        complexity: '3-5 weeks, requires PySpark expertise + DataOps experience + CI/CD pipeline skills',
        prerequisites: 'Databricks Runtime 11.3+, Unity Catalog enabled, source data in Delta format, version control (Git)'
      },
      'MLflow': {
        steps: [
          '‚Ä¢ Configure MLflow Tracking Server with backend store: \`mlflow.set_tracking_uri("databricks")\` and artifact store in Unity Catalog volumes',
          '‚Ä¢ Implement experiment tracking with autologging: \`mlflow.autolog()\` for scikit-learn/XGBoost/TensorFlow with hyperparameter capture',
          '‚Ä¢ Register models in Unity Catalog: \`mlflow.register_model(f"runs:/{run_id}/model", "catalog.schema.model_name")\` with versioning and aliases',
          '‚Ä¢ Deploy to Model Serving: \`POST /api/2.0/serving-endpoints\` with \`{"served_models": [{"model_name": "...", "model_version": "1", "workload_size": "Small", "scale_to_zero_enabled": true}]}\`'
        ],
        complexity: '3-4 weeks, requires ML engineering + MLOps + Kubernetes/container knowledge',
        prerequisites: 'Databricks ML Runtime, Unity Catalog, Model Registry access, production endpoint permissions'
      },
      'Databricks SQL': {
        steps: [
          '‚Ä¢ Provision Serverless SQL Warehouse: \`POST /api/2.0/sql/warehouses\` with \`{"name": "...", "cluster_size": "2X-Small", "serverless": true, "enable_photon": true}\`',
          '‚Ä¢ Implement Liquid Clustering for query optimization: \`ALTER TABLE catalog.schema.table CLUSTER BY (column1, column2)\` for 2-5√ó performance improvement',
          '‚Ä¢ Configure Query Result Caching and Delta Caching for sub-second response times on repeated queries',
          '‚Ä¢ Set up SQL Alerts with Webhooks: \`POST /api/2.0/preview/sql/alerts\` with Slack/email notification on SLA breach or anomaly detection'
        ],
        complexity: '2-3 weeks, requires SQL optimization + data modeling + BI integration experience',
        prerequisites: 'Databricks SQL Pro/Serverless, Unity Catalog with SELECT grants, Photon enabled, Serverless GA (Q2 2024)'
      },
      'Model Serving': {
        steps: [
          '‚Ä¢ Deploy Serverless Model Serving endpoint: \`POST /api/2.0/serving-endpoints\` with auto-scaling (0-10 instances) and GPU support (A10G/A100)',
          '‚Ä¢ Configure AI Gateway for rate limiting, PII detection: \`POST /api/2.0/serving-endpoints/{name}/config\` with Lakehouse Monitoring integration',
          '‚Ä¢ Implement A/B testing with traffic splitting: Update endpoint config with \`{"traffic_config": {"routes": [{"served_model_name": "champion", "traffic_percentage": 90}, {"served_model_name": "challenger", "traffic_percentage": 10}]}}\`',
          '‚Ä¢ Monitor inference latency and drift: Query \`system.serving.inference_log\` and \`system.serving.request_log\` for real-time dashboards'
        ],
        complexity: '3-5 weeks, requires MLOps + distributed systems + monitoring/observability expertise',
        prerequisites: 'Databricks ML Runtime, registered models in Unity Catalog, production workspace, Serverless Compute GA'
      },
      'Workflows': {
        steps: [
          '‚Ä¢ Define multi-task DAG via Jobs API: \`POST /api/2.1/jobs/create\` with task dependencies, retry policies, timeout configurations',
          '‚Ä¢ Implement parameterized workflows with Widgets: \`dbutils.widgets.text("param", "default")\` and pass values via Job Parameters JSON',
          '‚Ä¢ Configure Git-based source control: Link workflow to GitHub/GitLab repo via \`git_source\` with branch/tag/commit SHA for reproducibility',
          '‚Ä¢ Set up alerting and SLA monitoring: \`POST /api/2.1/jobs/{job_id}/runs/submit\` with email/webhook notifications on failure, timeout, or SLA breach'
        ],
        complexity: '2-4 weeks, requires orchestration + DevOps + Git workflow expertise',
        prerequisites: 'Databricks Jobs access, Git integration enabled, service principal for automation, Unity Catalog for secrets'
      }
    };
    
    // Get technical implementation for primary feature, fallback to generic
    const implementation = technicalMappings[primaryFeature] || {
      steps: [
        `‚Ä¢ Deploy ${primaryFeature} via Databricks REST API (\`/api/2.1/...\`) with OAuth 2.0 service principal authentication`,
        `‚Ä¢ Configure ${secondaryFeature} integration using infrastructure-as-code (Databricks Asset Bundles or Terraform Provider)`,
        `‚Ä¢ Implement monitoring and observability with System Tables (\`system.compute.*\`, \`system.billing.*\`) and Lakehouse Monitoring`,
        `‚Ä¢ Enable CI/CD pipeline with automated testing (pytest), security scanning (SAST/DAST), and blue-green deployment`,
        `**Latest Features**: Serverless Compute (GA Q2 2024), Enhanced Autoscaling, System Tables for Billing/Governance`
      ],
      complexity: '3-6 weeks | Requires: Databricks Certified Associate + platform engineering + DevOps + cloud architecture',
      prerequisites: 'Account/Workspace admin, Unity Catalog, cloud IAM roles (AWS/Azure/GCP), Git repository, CI/CD platform'
    };
    
    return implementation;
  }
  
  /**
   * Pillar-specific default recommendations - TECHNICAL IMPLEMENTATION FOCUSED
   */
  pillarDefaultRecommendation(painPoint, pillarId) {
    const pillarDefaults = {
      'platform_governance': {
        features: ['Unity Catalog', 'Account Console', 'Audit Logs', 'Cluster Policies'],
        apiEndpoints: [
          '\`POST /api/2.1/unity-catalog/metastores\` - Deploy centralized metastore with multi-cloud storage',
          '\`POST /api/2.0/policies/clusters/create\` - Enforce compute governance with auto-termination and Photon',
          '\`GET /api/2.0/accounts/{account_id}/audit\` - Stream audit logs to SIEM (Splunk/DataDog) via System Tables',
          '\`POST /api/2.0/accounts/{account_id}/scim/v2/Groups\` - Automate RBAC with Azure AD/Okta SCIM integration'
        ],
        complexity: '5-7 weeks | Requires: Account Admin + Cloud IAM + Security/Compliance expertise',
        latestFeature: 'Attribute-Based Access Control (ABAC) - GA Q3 2024'
      },
      'data_engineering': {
        features: ['Delta Live Tables', 'Workflows', 'Auto Loader', 'Lakehouse Monitoring'],
        apiEndpoints: [
          '\`POST /api/2.0/pipelines\` - Deploy DLT with Serverless compute and Change Data Capture (CDC)',
          '\`POST /api/2.1/jobs/create\` - Orchestrate multi-task DAGs with conditional execution and retries',
          '\`POST /api/2.0/lakehouse-monitors\` - Monitor data quality with drift detection and automated alerting',
          '\`SELECT * FROM cloud_files("s3://bucket", "json")\` - Auto Loader for schema evolution and backfill'
        ],
        complexity: '4-6 weeks | Requires: PySpark + Databricks Runtime 13.3+ + DataOps/DevOps skills',
        latestFeature: 'DLT Serverless + Predictive I/O - GA Q1 2024'
      },
      'analytics_bi': {
        features: ['Databricks SQL', 'Photon', 'Genie', 'Liquid Clustering'],
        apiEndpoints: [
          '\`POST /api/2.0/sql/warehouses\` - Provision Serverless SQL with auto-scaling (2X-Small to 4X-Large)',
          '\`ALTER TABLE catalog.schema.table CLUSTER BY (col1, col2)\` - Implement Liquid Clustering for 2-5√ó query speedup',
          '\`POST /api/2.0/preview/genie/spaces\` - Deploy Genie AI analyst with natural language query interface',
          '\`POST /api/2.0/sql/dashboards\` - Automate dashboard refresh with scheduled queries and Slack alerts'
        ],
        complexity: '3-5 weeks | Requires: SQL tuning + Unity Catalog + BI tool integration (Tableau/Power BI)',
        latestFeature: 'Genie (AI Analyst) + Serverless SQL GA - Q2 2024'
      },
      'machine_learning': {
        features: ['MLflow', 'Feature Store', 'Model Serving', 'Lakehouse Monitoring'],
        apiEndpoints: [
          '\`mlflow.register_model("runs:/{run_id}/model", "catalog.schema.model")\` - Version models in Unity Catalog',
          '\`POST /api/2.0/serving-endpoints\` - Deploy Serverless Model Serving with GPU (A10G) and auto-scaling',
          '\`POST /api/2.0/feature-store/feature-tables\` - Centralize features with Online Tables for <10ms serving',
          '\`POST /api/2.0/lakehouse-monitors\` - Monitor model drift, data quality, and inference latency in production'
        ],
        complexity: '4-6 weeks | Requires: ML Engineering + MLOps + Kubernetes + monitoring expertise',
        latestFeature: 'Serverless Model Serving + Lakehouse Monitoring for ML - GA Q1 2024'
      },
      'generative_ai': {
        features: ['Mosaic AI Agent Framework', 'Vector Search', 'Foundation Model APIs', 'AI Gateway'],
        apiEndpoints: [
          '\`POST /api/2.0/vector-search/indexes\` - Deploy hybrid search with embedding models (BGE, OpenAI, Cohere)',
          '\`mlflow.langchain.log_model()\` - Register RAG chain in Model Registry with Agent Evaluation framework',
          '\`POST /api/2.0/serving-endpoints/{name}/invocations\` - Serve Foundation Models (Llama 3, DBRX, Mixtral) via AI Gateway',
          '\`POST /api/2.0/serving-endpoints/{name}/config\` - Configure rate limiting, PII detection, prompt firewall'
        ],
        complexity: '5-8 weeks | Requires: LLM expertise + Vector DB + RAG architecture + Python SDK',
        latestFeature: 'Mosaic AI Agent Framework + Agent Evaluation - GA Q3 2024'
      },
      'operational_excellence': {
        features: ['Unity Catalog', 'System Tables', 'Databricks Academy', 'Asset Bundles'],
        apiEndpoints: [
          '\`SELECT * FROM system.compute.clusters WHERE date >= current_date() - 30\` - Analyze compute utilization',
          '\`databricks bundle deploy\` - Deploy infrastructure-as-code with Asset Bundles (YAML + Git)',
          '\`POST /api/2.0/workspace/import\` - Automate notebook deployment with CI/CD (GitHub Actions)',
          '\`GET /api/2.0/clusters/list\` - Audit cluster configurations for compliance and cost optimization'
        ],
        complexity: '3-5 weeks | Requires: Platform engineering + System Tables + Terraform + DevOps',
        latestFeature: 'Databricks Asset Bundles (DABs) + Enhanced System Tables - GA Q4 2023'
      }
    };
    
    const defaults = pillarDefaults[pillarId] || pillarDefaults['platform_governance'];
    
    return {
      problem: painPoint.label,
      solution: `Deploy ${defaults.features.join(', ')} with production-grade architecture, security, and monitoring. Implementation requires deep Databricks platform expertise and infrastructure-as-code patterns.`,
      recommendations: [
        `**Technical Architecture for ${painPoint.label}**:`,
        ...defaults.apiEndpoints,
      ],
      databricks_features: defaults.features
      // Note: High-level Next Steps are handled by buildContextualNextSteps() using nextStepsLibrary
    };
  }
  
  /**
   * Generate intelligent recommendations for a pillar
   * NOW WITH DATABASE INTEGRATION! üöÄ
   */
  async generateRecommendations(assessment, pillarId, pillarFramework) {
    console.log(`[IntelligentEngine V2] Analyzing pillar: ${pillarId}`);
    
    const responses = assessment.responses || {};
    const painPoints = this.extractPainPoints(responses, pillarFramework);
    const comments = this.extractComments(responses, pillarFramework);
    const stateGaps = this.analyzeStateGaps(responses, pillarFramework);
    
    console.log(`[IntelligentEngine V2] Pain points: ${painPoints.length}, Comments: ${comments.length}, State gaps: ${stateGaps.length}`);
    console.log(`[IntelligentEngine V2] Pillar: ${pillarId}, Sample pain points:`, painPoints.slice(0, 3).map(p => p.value));
    
    // Generate recommendations for each pain point
    const allRecommendations = [];
    const allNextSteps = [];
    const featureSet = new Set();
    
    // Prioritize pain points by score (severity)
    const topPainPoints = painPoints
      .sort((a, b) => (b.score || 3) - (a.score || 3))
      .slice(0, 10); // Top 10 most critical
    
    // üöÄ QUERY DATABASE FOR FEATURES THAT SOLVE THESE PAIN POINTS
    const dbFeatures = await this.queryDatabaseFeatures(topPainPoints, pillarId);
    console.log(`[IntelligentEngine V2] üéØ Found ${dbFeatures.length} features from database for ${topPainPoints.length} pain points`);
    
    // üî• NEW: USE DATABASE FEATURES TO GENERATE ALL CONTENT
    if (dbFeatures.length > 0) {
      console.log(`[IntelligentEngine V2] üöÄ USING DATABASE for ALL recommendation content`);
      
      // Generate recommendations from database features
      for (const feature of dbFeatures) {
        // Get full feature details including technical info and benefits
        const featureDetails = await this.getFeatureDetails(feature.id);
        
        if (featureDetails) {
          // Build recommendation from database data
          const recommendation = await this.buildRecommendationFromDatabase(feature, featureDetails);
          if (recommendation) {
            allRecommendations.push(recommendation);
          }
          
          // Collect feature name
          featureSet.add(feature.name);
        }
      }
      
      // üéØ Generate contextual, non-repetitive next steps based on full assessment context
      console.log(`[IntelligentEngine V2] üéØ Calling buildContextualNextSteps for ${pillarId}...`);
      const contextualNextSteps = await this.buildContextualNextSteps(
        assessment,
        pillarId,
        painPoints,
        comments,
        stateGaps,
        dbFeatures
      );
      console.log(`[IntelligentEngine V2] üì¶ buildContextualNextSteps returned:`, contextualNextSteps);
      allNextSteps.push(...contextualNextSteps);
      
      console.log(`[IntelligentEngine V2] ‚úÖ Generated ${allRecommendations.length} recommendations from DATABASE`);
      console.log(`[IntelligentEngine V2] ‚úÖ Generated ${contextualNextSteps.length} contextual next steps`);
      console.log(`[IntelligentEngine V2] üìä allNextSteps now contains ${allNextSteps.length} items`);
      
    } else {
      console.log(`[IntelligentEngine V2] ‚ö†Ô∏è No database features, falling back to hardcoded solutionMap`);
      
      // FALLBACK: Use hardcoded solutionMap only if database has no results
      for (const painPoint of topPainPoints) {
        let solution = this.solutionMap[painPoint.value];
        
        if (!solution) {
          solution = this.smartFallback(painPoint, pillarId);
        }
        
        if (solution) {
          if (solution.recommendations) {
            allRecommendations.push(...solution.recommendations);
          }
          if (solution.next_steps) {
            allNextSteps.push(...solution.next_steps);
          }
          if (solution.databricks_features) {
            solution.databricks_features.forEach(f => featureSet.add(f));
          }
        }
      }
      
      // üéØ ALWAYS generate contextual next steps from nextStepsLibrary (even without database features)
      console.log(`[IntelligentEngine V2] üéØ Calling buildContextualNextSteps for ${pillarId} (fallback path)...`);
      const contextualNextSteps = await this.buildContextualNextSteps(
        assessment,
        pillarId,
        painPoints,
        comments,
        stateGaps,
        [] // No database features, but we still have the nextStepsLibrary
      );
      console.log(`[IntelligentEngine V2] üì¶ buildContextualNextSteps returned:`, contextualNextSteps);
      allNextSteps.push(...contextualNextSteps);
      console.log(`[IntelligentEngine V2] ‚úÖ Generated ${contextualNextSteps.length} contextual next steps (fallback path)`);
    }
    
    // üéØ Generate context-aware strengths and challenges
    const theGood = this.buildContextualStrengths(assessment, pillarId, painPoints, comments, stateGaps, pillarFramework);
    const theBad = this.buildContextualChallenges(assessment, pillarId, topPainPoints, comments, stateGaps);
    
    // üöÄ BUILD DATABRICKS FEATURES from database
    let painPointFeatures;
    if (dbFeatures.length > 0) {
      console.log(`[IntelligentEngine V2] ‚úÖ Building ${dbFeatures.length} Databricks Features from DATABASE`);
      painPointFeatures = dbFeatures.map(f => ({
        name: f.name,
        description: f.description || f.short_description,
        benefits: f.benefits || [`GA: ${f.ga_status}, Released: ${f.ga_quarter}`],
        docsLink: f.docs || f.documentation_url
      }));
      
      // üéØ FILTER OUT CROSS-PILLAR CONTAMINATION
      // Remove features that don't belong to this pillar
      const genAIFeatures = ['AI Gateway', 'Vector Search', 'Mosaic AI', 'Foundation Model', 'AI Playground', 'DBRX', 'AI Functions', 'Agent Framework'];
      const mlFeatures = ['MLflow', 'Feature Store', 'Model Registry', 'Model Serving', 'AutoML'];
      
      if (pillarId === 'data_engineering') {
        // Data Engineering should NOT have GenAI or ML features
        painPointFeatures = painPointFeatures.filter(f => 
          !genAIFeatures.some(genAI => f.name.includes(genAI)) &&
          !mlFeatures.some(ml => f.name.includes(ml))
        );
        console.log(`[IntelligentEngine V2] üßπ Filtered out GenAI/ML features, ${painPointFeatures.length} features remaining for ${pillarId}`);
      } else if (pillarId === 'analytics_bi') {
        // Analytics should NOT have GenAI features
        painPointFeatures = painPointFeatures.filter(f => 
          !genAIFeatures.some(genAI => f.name.includes(genAI))
        );
        console.log(`[IntelligentEngine V2] üßπ Filtered out GenAI features, ${painPointFeatures.length} features remaining for ${pillarId}`);
      } else if (pillarId === 'platform_governance') {
        // Platform should NOT have GenAI-specific features (but can have Unity Catalog, Audit Logs, etc.)
        const genAIOnlyFeatures = ['AI Gateway', 'Vector Search', 'Mosaic AI', 'Foundation Model', 'AI Playground', 'DBRX', 'AI Functions', 'Agent Framework'];
        painPointFeatures = painPointFeatures.filter(f => 
          !genAIOnlyFeatures.some(genAI => f.name.includes(genAI))
        );
        console.log(`[IntelligentEngine V2] üßπ Filtered out GenAI-only features, ${painPointFeatures.length} features remaining for ${pillarId}`);
      }
    } else {
      console.log(`[IntelligentEngine V2] ‚ö†Ô∏è No database features, using hardcoded feature mapping`);
      painPointFeatures = this.mapPainPointsToFeatures(topPainPoints, pillarId);
    }
    
    console.log(`[IntelligentEngine V2] üìä FINAL COUNTS: ${allRecommendations.length} recommendations, ${allNextSteps.length} next steps, ${painPointFeatures.length} features`);
    console.log(`[IntelligentEngine V2] üìä What's Working: ${theGood.length}, Key Challenges: ${theBad.length}`);
    console.log(`[IntelligentEngine V2] üîç First 2 next steps for ${pillarId}:`, allNextSteps.slice(0, 2));
    
    return {
      theGood: theGood.slice(0, 5),
      theBad: theBad,
      recommendations: allRecommendations.slice(0, 8),
      nextSteps: allNextSteps,  // Return all contextual next steps (typically 5-7)
      databricksFeatures: painPointFeatures.slice(0, 4)
    };
  }

  /**
   * üöÄ NEW: Query database for features that solve pain points
   */
  async queryDatabaseFeatures(painPoints, pillarId) {
    try {
      const painPointValues = painPoints.map(p => p.value);
      console.log(`[IntelligentEngine V2] üîç Querying database for pain points:`, painPointValues.slice(0, 3));
      
      const features = await this.featureDB.getFeaturesForPainPoints(painPointValues, pillarId);
      
      console.log(`[IntelligentEngine V2] üìä Database returned ${features.length} features`);
      
      return features;
    } catch (error) {
      console.error(`[IntelligentEngine V2] ‚ùå Database query failed:`, error.message);
      return []; // Return empty array on error, fall back to hardcoded
    }
  }

  /**
   * üöÄ NEW: Get full feature details from database (including technical details and benefits)
   */
  async getFeatureDetails(featureId) {
    try {
      const details = await this.featureDB.getFeatureDetails(featureId);
      return details;
    } catch (error) {
      console.error(`[IntelligentEngine V2] ‚ùå Failed to get feature details for ID ${featureId}:`, error.message);
      return null;
    }
  }

  /**
   * üöÄ NEW: Build a recommendation object from database feature data
   */
  async buildRecommendationFromDatabase(feature, featureDetails) {
    try {
      // Extract benefits from featureDetails (array of benefit objects)
      let benefits = [];
      if (featureDetails && Array.isArray(featureDetails.benefits)) {
        benefits = featureDetails.benefits.map(b => b.benefit_description || b.description);
      }
      
      // Fallback benefits if none found
      if (benefits.length === 0) {
        benefits = [
          `Released: ${feature.ga_quarter || 'Latest'}`,
          `Category: ${feature.category || 'Enterprise'}`,
          `Status: ${feature.ga_status || 'GA'}`
        ];
      }

      // Build the recommendation structure
      const recommendation = {
        title: feature.name,
        description: feature.detailed_description || feature.description || 'Advanced Databricks capability to address your technical challenges',
        benefits: benefits,
        icon: this.getCategoryIcon(feature.category),
        docsLink: feature.documentation_url || feature.docs || 'https://docs.databricks.com',
        gaStatus: feature.ga_status || 'GA',
        releaseDate: feature.ga_quarter || 'Latest',
        category: feature.category,
        recommendationText: feature.recommendation_text // From pain point mapping
      };

      // Add technical details if available
      if (featureDetails && featureDetails.capabilities) {
        const tech = featureDetails.capabilities;
        recommendation.technicalDetails = {
          complexity: this.getComplexityFromWeeks(feature.complexity_weeks),
          prerequisites: tech.prerequisites || 'Databricks workspace with appropriate permissions',
          apiEndpoint: tech.api_endpoint,
          apiMethod: tech.api_method,
          configExample: tech.configuration_example,
          terraformResource: tech.terraform_resource,
          databricksCLI: tech.databricks_cli_command
        };

        // Add implementation steps if available
        if (Array.isArray(featureDetails.implementation_steps) && featureDetails.implementation_steps.length > 0) {
          recommendation.implementationSteps = featureDetails.implementation_steps.map(s => ({
            order: s.step_order,
            title: s.step_title,
            description: s.step_description,
            estimatedHours: s.estimated_hours,
            skillRequired: s.skill_required
          }));
        }
      }

      console.log(`[IntelligentEngine V2] ‚úÖ Built recommendation for: ${feature.name}`);
      return recommendation;

    } catch (error) {
      console.error(`[IntelligentEngine V2] ‚ùå Failed to build recommendation from feature ${feature.name}:`, error.message);
      console.error(`[IntelligentEngine V2] Stack:`, error.stack);
      return null;
    }
  }

  /**
   * Helper: Convert complexity weeks to label
   */
  getComplexityFromWeeks(weeks) {
    if (!weeks) return 'Medium';
    if (weeks <= 2) return 'Low';
    if (weeks <= 4) return 'Medium';
    if (weeks <= 8) return 'High';
    return 'Very High';
  }

  /**
   * üéØ Build specific, relevant next steps from comprehensive list
   * Selects actionable items tailored to each pillar
   */
  async buildContextualNextSteps(assessment, pillarId, painPoints, comments, stateGaps, features) {
    // Calculate current and future average scores for decision logic
    const avgCurrent = stateGaps.length > 0 ? stateGaps.reduce((sum, g) => sum + g.current, 0) / stateGaps.length : 0;
    const avgGap = stateGaps.length > 0 ? (stateGaps.reduce((sum, g) => sum + g.future, 0) / stateGaps.length) - avgCurrent : 0;
    
    // Comprehensive next steps library by pillar - Simplified format with arrows
    const nextStepsLibrary = {
      platform_governance: [
        'Executive Alignment: Present governance vision to leadership; secure funding and sponsorship for cross-domain rollout',
        'Workshop: Conduct a Platform Governance & Unity Catalog Workshop with Databricks SMEs to align architecture, access models, and catalog strategy',
        'Enablement & Training: Role-based sessions for platform admins and data stewards on workspace hierarchy, permissions, lineage, and FinOps tagging',
        'Adoption Strategy: Start with one business domain as a pilot (e.g., Finance, Clinical, or Claims), then scale horizontally',
        'Assessment: Run a Security, Compliance & Governance Assessment to benchmark against HIPAA, HITRUST, and GxP best practices',
        'Industry Outlook: Review emerging regulations (e.g., AI Act, Data Privacy) and benchmark governance models across healthcare peers',
        'Partner / SI Engagement: Engage SI partners (e.g., Deloitte, Slalom, Accenture) for governance rollout and cross-workspace architecture design',
        'Change Management: Create a governance council; publish policies, catalog taxonomy, and data ownership matrix'
      ],
      data_engineering: [
        'Workshop: Host a LakeFlow / Delta Live Tables Ingestion Strategy Workshop to align on ingestion patterns, SLAs, and DQ policies',
        'Enablement: Train engineers and analysts on medallion architecture principles, CDC integration, and cost-efficient job scheduling',
        'Adoption & Pilot: Identify 1-2 high-value pipelines for modernization (e.g., Claims ETL or Member 360 ingestion) as lighthouse examples',
        'Assessment: Conduct a Pipeline Reliability & Cost Optimization Assessment to quantify gains from migration',
        'Industry Outlook: Highlight interoperability trends ‚Äî FHIR, HL7, and real-time event streaming for healthcare analytics',
        'Partner / SI Engagement: Leverage ETL modernization partners (e.g., TCS, Cognizant, Wipro) for workload migration support',
        'Governance Alignment: Define data ownership and stewardship roles per pipeline domain',
        'Change Management: Create documentation and reusable templates for new pipelines; standardize intake and approval workflows'
      ],
      analytics_bi: [
        'Workshop: Run a Modern BI on Lakehouse Workshop (Databricks SQL + Power BI) to align dashboard strategy',
        'Enablement: Train business users on dataset discovery via Unity Catalog and certified data usage',
        'Adoption: Identify top 5 dashboards to migrate; ensure alignment with governed datasets and performance SLAs',
        'Assessment: Execute a BI Performance & Adoption Assessment to evaluate latency, concurrency, and usability',
        'Industry Outlook: Share BI modernization success stories (payer/provider data democratization)',
        'Partner / SI Engagement: Collaborate with BI accelerators (e.g., Thorogood, Tredence, Aimpoint Digital) for dashboard modernization',
        'Change Management: Establish "Data Champions" in each department to drive adoption and training',
        'Success Metrics: Measure report refresh frequency, active dashboard usage, and time-to-insight KPIs'
      ],
      machine_learning: [
        'Workshop: Conduct a Model Lifecycle Management Workshop covering MLflow, Model Registry, and MLOps governance',
        'Enablement: Provide training for DS/ML teams on model versioning, experiment tracking, and deployment workflows',
        'Adoption: Select one business use case (e.g., fraud detection, churn prediction, or risk scoring) for MLflow deployment pilot',
        'Assessment: Perform an ML Governance & Readiness Assessment focusing on model lineage, explainability, and audit',
        'Industry Outlook: Review state of MLOps adoption and regulated AI compliance in healthcare and life sciences',
        'Partner / SI Engagement: Engage AI partners (ZS, IQVIA, Deloitte AI) for co-development or validation support',
        'Change Management: Form an internal "Model Review Board" to standardize approval and transition criteria',
        'Measurement: Track metrics like model re-training frequency, deployment cycle time, and business ROI'
      ],
      generative_ai: [
        'Workshop: Schedule a GenAI Strategy & AI Gateway Workshop to define architecture, security, and governance approach',
        'Enablement: Conduct enablement sessions for developers and product owners on RAG, prompt engineering, and evaluation frameworks',
        'Adoption: Identify 2‚Äì3 lighthouse GenAI use cases (e.g., Prior Auth Optimization, Intelligent Documentation, Customer Support Copilot)',
        'Assessment: Perform a GenAI Readiness & Risk Assessment (focus: privacy, bias, and model auditability)',
        'Industry Outlook: Present evolving trends ‚Äî GenAI copilots, regulated model governance, fine-tuning ethics, and AI act implications',
        'Partner / SI Engagement: Collaborate with GenAI implementation partners (Quantiphi, Databricks PS, or industry ISVs)',
        'Change Management: Define a responsible-AI policy; involve legal, compliance, and data ethics teams early',
        'Measurement: Track adoption metrics: user satisfaction, productivity gains, task automation rates'
      ],
      operational_excellence: [
        'Workshop: Conduct Ops & FinOps Workshop ‚Äî focus on system tables, cost optimization, audit logs, and SLA reporting',
        'Enablement: Train administrators and PMs on monitoring, alerting, tagging, and usage governance',
        'Adoption: Standardize ops dashboards and monthly business reviews (MBRs) to track system health and cost',
        'Assessment: Run an Operational Maturity Assessment on incident management, SLAs, and observability',
        'Industry Outlook: Benchmark operational cost, uptime, and governance KPIs vs. peers',
        'Partner / SI Engagement: Collaborate with managed service providers (e.g., Persistent, Capgemini, HCL) for 24√ó7 support',
        'Change Management: Define escalation paths, ticket SLAs, and accountability models for teams',
        'Measurement: KPIs ‚Äî uptime %, mean time to detect (MTTD), cost reduction %, and automation coverage'
      ]
    };
    
    // Get pillar-specific next steps
    const pillarNextSteps = nextStepsLibrary[pillarId] || nextStepsLibrary.operational_excellence;
    
    // üéØ DYNAMIC: Determine how many next steps to show based on maturity gap
    // Larger gaps = more complex transformation = more next steps needed
    let maxNextSteps = 4; // Default for small gaps (gap <= 1)
    
    if (avgGap > 3) {
      maxNextSteps = 8; // Show ALL steps for very large gaps (major transformation)
    } else if (avgGap > 2) {
      maxNextSteps = 6; // Show most steps for large gaps (significant effort)
    } else if (avgGap > 1) {
      maxNextSteps = 5; // Show extra steps for medium gaps
    }
    
    console.log(`[IntelligentEngine V2] üìä Maturity gap: ${avgGap.toFixed(1)}, showing ${maxNextSteps} next steps for ${pillarId}`);
    
    // Select most relevant, actionable next steps in priority order
    const selectedNextSteps = [];
    
    // 1. Always start with Workshop (technical engagement with Databricks)
    const workshop = pillarNextSteps.find(step => step.includes('Workshop:'));
    if (workshop) selectedNextSteps.push(workshop);
    
    // 2. Include Assessment if maturity gap is significant (gap > 2)
    if (avgGap > 2) {
      const assessment = pillarNextSteps.find(step => step.includes('Assessment:'));
      if (assessment) selectedNextSteps.push(assessment);
    }
    
    // 3. Include Enablement/Training (capacity building)
    const enablement = pillarNextSteps.find(step => 
      step.includes('Enablement:') || 
      step.includes('Enablement &') ||
      step.includes('Training:')
    );
    if (enablement && selectedNextSteps.length < maxNextSteps) {
      selectedNextSteps.push(enablement);
    }
    
    // 4. Include Adoption/Pilot (practical implementation)
    const adoption = pillarNextSteps.find(step => 
      step.includes('Adoption:') || 
      step.includes('Adoption &') || 
      step.includes('Adoption Strategy:') ||
      step.includes('Pilot:')
    );
    
    if (adoption && !selectedNextSteps.includes(adoption) && selectedNextSteps.length < maxNextSteps) {
      selectedNextSteps.push(adoption);
    }
    
    // 5. Fill remaining slots with other relevant steps (Partner Engagement, Change Management, Industry Outlook, Measurement)
    const remainingSlots = maxNextSteps - selectedNextSteps.length;
    if (remainingSlots > 0) {
      const otherSteps = pillarNextSteps.filter(step => !selectedNextSteps.includes(step));
      selectedNextSteps.push(...otherSteps.slice(0, remainingSlots));
    }
    
    // Ensure uniqueness and respect dynamic limit
    const uniqueNextSteps = [...new Set(selectedNextSteps)].slice(0, maxNextSteps);
    
    console.log(`[IntelligentEngine V2] ‚úÖ Built ${uniqueNextSteps.length} actionable next steps for ${pillarId} (max: ${maxNextSteps})`);
    return uniqueNextSteps;
  }

  /**
   * üöÄ NEW: Build a next step from database feature data (DEPRECATED - use buildContextualNextSteps)
   */
  async buildNextStepFromDatabase(feature, featureDetails) {
    try {
      // Determine the type of next step based on complexity
      const complexity = featureDetails?.capabilities?.complexity || 'Medium';
      let actionType = 'Workshop';
      
      if (complexity === 'Low') {
        actionType = 'Quick Start Training';
      } else if (complexity === 'Medium') {
        actionType = 'Technical Workshop';
      } else if (complexity === 'High' || complexity === 'Very High') {
        actionType = 'Discovery Session + POC';
      }

      const duration = this.estimateDuration(complexity);
      const stakeholders = this.getStakeholders(feature.category);
      
      // Return a formatted string that the frontend can display
      const nextStepString = `${actionType} (${duration}) with ${stakeholders}: Implement ${feature.name} to address ${feature.category || 'technical'} challenges and maximize ROI`;

      console.log(`[IntelligentEngine V2] ‚úÖ Built next step for: ${feature.name} (${actionType})`);
      return nextStepString;

    } catch (error) {
      console.error(`[IntelligentEngine V2] ‚ùå Failed to build next step from feature ${feature.name}:`, error.message);
      return null;
    }
  }

  /**
   * Helper: Get icon for category
   */
  getCategoryIcon(category) {
    const iconMap = {
      'Data Engineering': '‚öôÔ∏è',
      'Analytics': 'üìä',
      'Machine Learning': 'ü§ñ',
      'Governance': 'üîí',
      'Platform': 'üèóÔ∏è',
      'Operational Excellence': 'üéØ',
      'MLOps': 'üîÑ',
      'Security': 'üõ°Ô∏è',
      'Performance': '‚ö°',
      'Cost Optimization': 'üí∞',
      'Collaboration': 'üë•'
    };
    return iconMap[category] || 'üöÄ';
  }

  /**
   * Helper: Estimate duration based on complexity
   */
  estimateDuration(complexity) {
    const durationMap = {
      'Low': '1-2 weeks',
      'Medium': '3-4 weeks',
      'High': '6-8 weeks',
      'Very High': '8-12 weeks'
    };
    return durationMap[complexity] || '4-6 weeks';
  }

  /**
   * Helper: Get stakeholders based on category
   */
  getStakeholders(category) {
    const stakeholderMap = {
      'Data Engineering': ['Data Engineering Team', 'Platform Team', 'Data Architects'],
      'Analytics': ['Analytics Team', 'Business Intelligence Team', 'Data Analysts'],
      'Machine Learning': ['Data Science Team', 'ML Engineering Team', 'Product Team'],
      'Governance': ['Security Team', 'Compliance Team', 'Data Governance Team'],
      'Platform': ['Platform Engineering Team', 'DevOps Team', 'Infrastructure Team'],
      'Operational Excellence': ['Engineering Leadership', 'DevOps Team', 'SRE Team'],
      'MLOps': ['ML Engineering Team', 'Data Science Team', 'DevOps Team'],
      'Security': ['Security Team', 'Compliance Team', 'Platform Team'],
      'Performance': ['Engineering Team', 'Platform Team', 'Performance Team'],
      'Cost Optimization': ['FinOps Team', 'Engineering Leadership', 'Platform Team'],
      'Collaboration': ['All Technical Teams', 'Product Management', 'Engineering Leadership']
    };
    return stakeholderMap[category] || ['Engineering Team', 'Product Team', 'Technical Leadership'];
  }
  
  mapPainPointsToFeatures(painPoints, pillarId) {
    // Map pain points to specific Databricks features that solve them
    const featureMap = {
      // Platform Governance pain points
      'poor_isolation': ['Unity Catalog', 'Workspace Administration', 'Private Link', 'IP Access Lists'],
      'quality_issues': ['Delta Live Tables', 'Lakehouse Monitoring', 'Data Quality Checks'],
      'compliance_gaps': ['Unity Catalog', 'Audit Logs', 'Compliance Security Profile', 'Data Classification'],
      'compliance_risks': ['Unity Catalog', 'Audit Logs', 'Compliance Security Profile'],
      'weak_access_control': ['Unity Catalog', 'Service Principals', 'Attribute-Based Access Control'],
      'no_audit_logs': ['Audit Logs', 'System Tables', 'Unity Catalog Audit'],
      'manual_provisioning': ['Databricks Asset Bundles', 'Terraform Provider', 'Databricks CLI'],
      'no_iac': ['Databricks Asset Bundles', 'Terraform Provider', 'Databricks CLI'],
      'resource_conflicts': ['Workspace Administration', 'Cluster Policies', 'Budget Alerts'],
      
      // Data Engineering pain points
      'poor_quality': ['Delta Live Tables', 'Lakehouse Monitoring', 'Auto Loader'],
      'pipeline_failures': ['Delta Live Tables', 'Workflows', 'Alerting'],
      'manual_pipelines': ['Delta Live Tables', 'Workflows', 'Databricks Jobs'],
      'no_monitoring': ['Lakehouse Monitoring', 'System Tables', 'Delta Live Tables Event Log'],
      'scattered_data': ['Unity Catalog', 'Delta Sharing', 'Data Discovery'],
      'error_handling': ['Delta Live Tables', 'Workflows', 'Alerting'],
      'ingestion_issues': ['Auto Loader', 'Copy Into', 'Streaming Tables'],
      
      // Analytics & BI pain points
      'slow_queries': ['Photon', 'Liquid Clustering', 'Predictive I/O', 'Serverless SQL'],
      'inconsistent_performance': ['Serverless SQL', 'Photon', 'Result Caching'],
      'no_caching': ['Result Caching', 'Delta Caching', 'Disk Caching'],
      'limited_monitoring': ['Query History', 'Query Profile', 'System Tables'],
      'access_bottlenecks': ['SQL Warehouses', 'Serverless SQL', 'Partner Connect'],
      'loss_of_trust': ['Unity Catalog', 'Lakehouse Monitoring', 'Data Lineage', 'Audit Logs'],
      'conflicting_reports': ['Unity Catalog', 'Data Lineage', 'Semantic Layer'],
      'conflicting_kpis': ['Unity Catalog', 'Semantic Layer', 'SQL Warehouses'],
      'data_misuse': ['Unity Catalog', 'Audit Logs', 'Row-Level Security', 'Column Masking'],
      'regulatory_exposure': ['Unity Catalog', 'Compliance Security Profile', 'Audit Logs', 'Data Classification'],
      'compliance_sharing': ['Delta Sharing', 'Unity Catalog', 'Audit Logs', 'Clean Rooms'],
      
      // Machine Learning pain points
      'no_experiment_tracking': ['MLflow Tracking', 'MLflow Autologging', 'Experiments'],
      'no_model_registry': ['MLflow Model Registry', 'Unity Catalog for Models'],
      'scattered_artifacts': ['MLflow Model Registry', 'Unity Catalog'],
      'no_model_monitoring': ['Lakehouse Monitoring', 'Model Serving Metrics'],
      'manual_retraining': ['MLflow Webhooks', 'Workflows', 'AutoML'],
      'no_feature_store': ['Feature Store', 'Online Tables', 'Feature Serving'],
      'model_failures': ['Model Serving', 'Lakehouse Monitoring', 'AI Gateway'],
      
      // GenAI pain points
      'no_genai_strategy': ['AI Playground', 'Mosaic AI Agent Framework', 'AI Functions'],
      'unclear_use_cases': ['AI Playground', 'Solution Accelerators', 'Vector Search'],
      'no_vector_search': ['Vector Search', 'Online Tables', 'Delta Sync'],
      'prompt_management': ['AI Playground', 'MLflow', 'AI Gateway'],
      'no_rag': ['Mosaic AI Agent Framework', 'Vector Search'],
      'reputation_risk': ['AI Gateway', 'Guardrails', 'Content Filtering'],
      'compliance_risk': ['AI Gateway', 'Audit Logs', 'Model Monitoring'],
      
      // Operational Excellence pain points
      'no_coe': ['Databricks Academy', 'Professional Services', 'Partner Network'],
      'unclear_charter': ['Unity Catalog', 'System Tables', 'Account Console'],
      'resource_constraints': ['Databricks Academy', 'Professional Services', 'Community'],
      'standards_gaps': ['Databricks Asset Bundles', 'Cluster Policies', 'Unity Catalog'],
      'adoption_challenges': ['Databricks Assistant', 'Solution Accelerators'],
      'no_training': ['Databricks Academy', 'Partner Training'],
      'poor_collaboration': ['Repos', 'Git Integration', 'Comments'],
      'no_cost_tracking': ['System Tables', 'Budget Alerts']
    };
    
    const features = new Set();
    
    // For each pain point, add relevant features
    painPoints.forEach(pp => {
      const relevantFeatures = featureMap[pp.value] || [];
      relevantFeatures.slice(0, 2).forEach(f => features.add(f)); // Top 2 per pain point
    });
    
    // Convert feature names to objects with details
    const featureArray = Array.from(features);
    const featureDetails = [];
    
    // Get feature details from the feature mapper
    const currentScore = 3;
    const pillarRecs = this.featureMapper.getRecommendationsForPillar(pillarId, currentScore, {});
    const allMapperFeatures = [
      ...(pillarRecs?.currentMaturity?.features || []),
      ...(pillarRecs?.nextLevel?.features || [])
    ];
    
    // Match our pain-point-specific features with detailed info from mapper
    featureArray.forEach(featureName => {
      const detailedFeature = allMapperFeatures.find(f => 
        f.name && (
          f.name.toLowerCase().includes(featureName.toLowerCase()) ||
          featureName.toLowerCase().includes(f.name.toLowerCase())
        )
      );
      
      if (detailedFeature) {
        featureDetails.push(detailedFeature);
      } else {
        // Create basic feature object with proper docs link
        const docsLink = this.getFeatureDocsLink(featureName);
        featureDetails.push({
          name: featureName,
          description: this.getFeatureDescription(featureName),
          benefits: this.getFeatureBenefits(featureName),
          docs: docsLink
        });
      }
    });
    
    console.log(`[mapPainPointsToFeatures] ${pillarId}: ${painPoints.length} pain points ‚Üí ${features.size} unique features ‚Üí ${featureDetails.length} with details`);
    
    return featureDetails;
  }
  
  getFeatureDocsLink(featureName) {
    const docsLinks = {
      'Unity Catalog': 'https://docs.databricks.com/en/data-governance/unity-catalog/index.html',
      'Audit Logs': 'https://docs.databricks.com/en/admin/account-settings/audit-logs.html',
      'Delta Sharing': 'https://docs.databricks.com/en/data-sharing/index.html',
      'Delta Live Tables': 'https://docs.databricks.com/en/delta-live-tables/index.html',
      'Lakehouse Monitoring': 'https://docs.databricks.com/en/lakehouse-monitoring/index.html',
      'Photon': 'https://docs.databricks.com/en/compute/photon.html',
      'Serverless SQL': 'https://docs.databricks.com/en/sql/admin/serverless.html',
      'Liquid Clustering': 'https://docs.databricks.com/en/delta/clustering.html',
      'MLflow': 'https://docs.databricks.com/en/mlflow/index.html',
      'Model Serving': 'https://docs.databricks.com/en/machine-learning/model-serving/index.html',
      'Vector Search': 'https://docs.databricks.com/en/generative-ai/vector-search.html',
      'AI Gateway': 'https://docs.databricks.com/en/generative-ai/ai-gateway.html',
      'Workflows': 'https://docs.databricks.com/en/jobs/index.html',
      'Auto Loader': 'https://docs.databricks.com/en/ingestion/auto-loader/index.html',
      'Databricks Asset Bundles': 'https://docs.databricks.com/en/dev-tools/bundles/index.html',
      'Feature Store': 'https://docs.databricks.com/en/machine-learning/feature-store/index.html',
      'Databricks Academy': 'https://www.databricks.com/learn/training',
      'System Tables': 'https://docs.databricks.com/en/admin/system-tables/index.html',
      'Cluster Policies': 'https://docs.databricks.com/en/admin/clusters/policies.html',
      'Row-Level Security': 'https://docs.databricks.com/en/data-governance/unity-catalog/row-and-column-filters.html',
      'Column Masking': 'https://docs.databricks.com/en/data-governance/unity-catalog/row-and-column-filters.html',
      'Data Lineage': 'https://docs.databricks.com/en/data-governance/unity-catalog/data-lineage.html',
      'Data Classification': 'https://docs.databricks.com/en/data-governance/unity-catalog/tags.html',
      'Clean Rooms': 'https://docs.databricks.com/en/data-sharing/clean-rooms.html'
    };
    return docsLinks[featureName] || 'https://docs.databricks.com/en/index.html';
  }
  
  getFeatureDescription(featureName) {
    const descriptions = {
      'Unity Catalog': 'Unified governance for data and AI assets with centralized access control',
      'Audit Logs': 'Track and monitor all workspace activities for compliance and security',
      'Delta Sharing': 'Securely share live data across organizations without copying',
      'Delta Live Tables': 'Declarative ETL framework with automatic quality monitoring',
      'Lakehouse Monitoring': 'Monitor data quality, schema changes, and model performance',
      'Photon': 'High-performance query engine for 3-5x faster SQL and DataFrame queries',
      'Serverless SQL': 'Auto-scaling SQL warehouses with instant start and pay-per-query pricing',
      'Liquid Clustering': 'Automatic data layout optimization without manual Z-ordering',
      'MLflow': 'Open-source platform for ML lifecycle management',
      'Model Serving': 'Deploy models as REST APIs with auto-scaling and monitoring',
      'Vector Search': 'Managed vector database for similarity search and RAG applications',
      'AI Gateway': 'Centralized gateway for LLM usage with governance and cost tracking',
      'Workflows': 'Orchestrate data pipelines and ML workflows with dependencies',
      'Auto Loader': 'Incremental data ingestion with schema evolution and automatic recovery',
      'Databricks Asset Bundles': 'Infrastructure as code for Databricks resources',
      'Feature Store': 'Centralized repository for ML features with point-in-time lookups',
      'Databricks Academy': 'Free online training and certification programs',
      'System Tables': 'Queryable logs for usage, billing, and audit data',
      'Cluster Policies': 'Governance rules to control cluster configurations and costs',
      'Row-Level Security': 'Fine-grained access control at the row level',
      'Column Masking': 'Dynamic data masking for sensitive columns',
      'Data Lineage': 'Automatic tracking of data transformations and dependencies',
      'Data Classification': 'Tag and classify data for governance and compliance',
      'Clean Rooms': 'Collaborate on sensitive data without revealing raw data'
    };
    return descriptions[featureName] || `${featureName} capability for Databricks`;
  }
  
  getFeatureBenefits(featureName) {
    const benefits = {
      'Unity Catalog': ['Centralized governance', 'Fine-grained access control', 'Data lineage', 'Cross-workspace sharing'],
      'Audit Logs': ['Compliance tracking', 'Security monitoring', 'Activity history', 'Forensics'],
      'Delta Sharing': ['Secure data sharing', 'No data duplication', 'Live data access', 'Cross-organization collaboration'],
      'Delta Live Tables': ['Automated quality checks', 'Declarative pipelines', 'Error handling', 'Data lineage'],
      'Lakehouse Monitoring': ['Data quality monitoring', 'Drift detection', 'Automated alerting', 'Historical tracking'],
      'Photon': ['3-5x faster queries', 'Lower costs', 'No code changes', 'Automatic optimization'],
      'Serverless SQL': ['Instant start', 'Auto-scaling', 'Pay-per-query', 'No cluster management'],
      'Row-Level Security': ['Fine-grained access', 'Dynamic filtering', 'Compliance', 'Data privacy'],
      'Column Masking': ['Sensitive data protection', 'Automatic masking', 'Policy-based', 'Regulatory compliance'],
      'Data Lineage': ['Impact analysis', 'Compliance reporting', 'Data discovery', 'Debugging'],
      'Data Classification': ['Automated tagging', 'Compliance tracking', 'Data discovery', 'Governance policies'],
      'Clean Rooms': ['Privacy-preserving analytics', 'Secure collaboration', 'No raw data exposure', 'Regulatory compliance']
    };
    return benefits[featureName] || ['Addresses identified challenges', 'Databricks managed service'];
  }
  
  /**
   * Generate dynamic strategic roadmap based on assessment priorities and gaps
   */
  generateStrategicRoadmap(prioritizedActions) {
    console.log('[generateStrategicRoadmap] Generating roadmap from', prioritizedActions.length, 'pillars');
    
    // Sort by priority and gap
    const sorted = [...prioritizedActions].sort((a, b) => {
      // Critical first, then high, then medium/low
      const priorityOrder = { 'critical': 4, 'high': 3, 'medium': 2, 'low': 1 };
      if (priorityOrder[b.priority] !== priorityOrder[a.priority]) {
        return priorityOrder[b.priority] - priorityOrder[a.priority];
      }
      return b.gap - a.gap;
    });
    
    // Phase 1 (0-3 months): Foundation - Critical items + Unity Catalog (foundational)
    const phase1Items = [];
    const phase2Items = [];
    const phase3Items = [];
    
    // Always include Unity Catalog if there are governance/platform challenges
    const hasGovernanceChallenges = sorted.some(p => 
      p.pillarId === 'platform_governance' || 
      p.theBad?.some(challenge => 
        challenge.toLowerCase().includes('governance') ||
        challenge.toLowerCase().includes('access control') ||
        challenge.toLowerCase().includes('compliance')
      )
    );
    
    if (hasGovernanceChallenges) {
      phase1Items.push('Implement Unity Catalog for centralized governance - reducing compliance risk and manual access management');
    }
    
    // Phase 1: Critical priority pillars (gap >= 2)
    const criticalPillars = sorted.filter(p => p.gap >= 2);
    
    // Business outcome mapping for pillars
    const businessOutcomes = {
      'platform_governance': 'improving security posture and regulatory compliance',
      'data_engineering': 'reducing data delivery time and improving data quality',
      'analytics_bi': 'accelerating decision-making speed by 3-5√ó',
      'machine_learning': 'reducing model deployment time by 60-70%',
      'generative_ai': 'unlocking new AI-powered revenue streams',
      'operational_excellence': 'reducing operational overhead by 20-30%'
    };
    
    criticalPillars.slice(0, 2).forEach(pillar => {
      const topChallenge = pillar.theBad?.[0] || `Maturity gap of ${pillar.gap} levels`;
      const topFeature = pillar.databricksFeatures?.[0]?.name || 'recommended features';
      const outcome = businessOutcomes[pillar.pillarId] || 'driving measurable business value';
      const challengeShort = topChallenge.substring(0, 50);
      phase1Items.push(`${pillar.pillarName}: Deploy ${topFeature} to address ${challengeShort}... - ${outcome}`);
    });
    
    // Ensure we have at least 3 items in Phase 1
    if (phase1Items.length < 3 && sorted.length > 0) {
      const nextPillar = sorted.find(p => !criticalPillars.includes(p));
      if (nextPillar) {
        phase1Items.push(`Establish ${nextPillar.pillarName} monitoring and baseline metrics - enabling data-driven decision making`);
      }
    }
    
    // Phase 2: High priority pillars (gap = 1) + scale critical solutions
    const highPriorityPillars = sorted.filter(p => p.gap === 1 || (p.gap >= 2 && !criticalPillars.slice(0, 2).includes(p)));
    highPriorityPillars.slice(0, 2).forEach(pillar => {
      const topFeature = pillar.databricksFeatures?.[1]?.name || pillar.databricksFeatures?.[0]?.name || 'capabilities';
      const outcome = businessOutcomes[pillar.pillarId] || 'improving operational efficiency';
      phase2Items.push(`${pillar.pillarName}: Scale ${topFeature} across teams and use cases - ${outcome}`);
    });
    
    // Add integration items for Phase 2
    if (sorted.length >= 3) {
      phase2Items.push('Integrate monitoring dashboards across all pillars - enabling proactive issue detection and reducing downtime');
    }
    
    // Ensure we have at least 3 items in Phase 2
    if (phase2Items.length < 3 && sorted.length > 0) {
      phase2Items.push('Deploy second wave of capabilities based on Phase 1 learnings - maximizing adoption and ROI');
    }
    
    // Phase 3: Optimization and remaining gaps
    const remainingPillars = sorted.filter(p => p.gap === 0 || (p.gap === 1 && !highPriorityPillars.slice(0, 2).includes(p)));
    
    // Add MLOps/CI-CD if ML pillar exists
    const mlPillar = sorted.find(p => p.pillarId === 'machine_learning');
    if (mlPillar) {
      phase3Items.push('Formalize MLOps CI/CD pipeline for automated model deployment - reducing time-to-production by 70%');
    }
    
    // Add GenAI if pillar exists
    const genaiPillar = sorted.find(p => p.pillarId === 'genai' || p.pillarId === 'generative_ai');
    if (genaiPillar) {
      const hasRAG = genaiPillar.theBad?.some(c => c.toLowerCase().includes('rag'));
      if (hasRAG) {
        phase3Items.push('Expand GenAI use cases with RAG implementation and vector search - unlocking new revenue opportunities');
      } else {
        phase3Items.push('Deploy production GenAI applications with governance guardrails - creating competitive differentiation');
      }
    }
    
    // Add data mesh if multiple pillars are mature
    const maturePillars = sorted.filter(p => p.currentScore >= 4);
    if (maturePillars.length >= 3) {
      phase3Items.push('Align data mesh principles with Unity Catalog - enabling domain-oriented data ownership and reducing data silos');
    }
    
    // Ensure we have at least 3 items in Phase 3
    if (phase3Items.length < 3) {
      phase3Items.push('Optimize and tune all deployed capabilities - maximizing ROI and reducing TCO by 15-25%');
      phase3Items.push('Establish center of excellence - ensuring sustainable adoption and continuous improvement');
    }
    
    const roadmap = {
      phases: [
        {
          id: 'phase1',
          title: 'Phase 1: Foundation (0‚Äì3 months)',
          items: phase1Items.slice(0, 3)
        },
        {
          id: 'phase2',
          title: 'Phase 2: Scale (3‚Äì6 months)',
          items: phase2Items.slice(0, 3)
        },
        {
          id: 'phase3',
          title: 'Phase 3: Optimize (6‚Äì12 months)',
          items: phase3Items.slice(0, 3)
        }
      ]
    };
    
    console.log('[generateStrategicRoadmap] Generated roadmap:', JSON.stringify(roadmap, null, 2));
    return roadmap;
  }
  
  /**
   * Calculate expected business impact based on assessment data
   */
  calculateBusinessImpact(assessment, prioritizedActions, industry = 'Technology') {
    console.log('[calculateBusinessImpact] Calculating for industry:', industry);
    
    const responses = assessment.responses || {};
    
    // Calculate average gap across all pillars
    const totalGap = prioritizedActions.reduce((sum, p) => sum + (p.gap || 0), 0);
    const avgGap = prioritizedActions.length > 0 ? totalGap / prioritizedActions.length : 0;
    
    // Calculate average current maturity
    const totalCurrent = prioritizedActions.reduce((sum, p) => sum + (p.currentScore || 3), 0);
    const avgCurrent = prioritizedActions.length > 0 ? totalCurrent / prioritizedActions.length : 3;
    
    // Calculate average target maturity
    const totalTarget = prioritizedActions.reduce((sum, p) => sum + (p.targetScore || 4), 0);
    const avgTarget = prioritizedActions.length > 0 ? totalTarget / prioritizedActions.length : 4;
    
    console.log(`[calculateBusinessImpact] Avg gap: ${avgGap.toFixed(1)}, Avg current: ${avgCurrent.toFixed(1)}, Avg target: ${avgTarget.toFixed(1)}`);
    
    // Industry-specific multipliers (based on data-driven insights)
    const industryMultipliers = {
      'Financial Services': { speed: 1.2, cost: 1.1, overhead: 1.15 },
      'Healthcare': { speed: 1.15, cost: 1.05, overhead: 1.2 },
      'Retail': { speed: 1.25, cost: 1.15, overhead: 1.1 },
      'Manufacturing': { speed: 1.1, cost: 1.2, overhead: 1.25 },
      'Technology': { speed: 1.15, cost: 1.1, overhead: 1.15 },
      'Telecommunications': { speed: 1.2, cost: 1.15, overhead: 1.2 },
      'Energy': { speed: 1.1, cost: 1.2, overhead: 1.15 },
      'default': { speed: 1.0, cost: 1.0, overhead: 1.0 }
    };
    
    const multipliers = industryMultipliers[industry] || industryMultipliers['default'];
    
    // Base impact calculations using industry benchmarks
    
    // 1. ANALYTICS SPEED (based on Analytics/BI and Data Engineering maturity improvements)
    const analyticsPillar = prioritizedActions.find(p => p.pillarId === 'analytics_bi');
    const dataEngPillar = prioritizedActions.find(p => p.pillarId === 'data_engineering');
    
    // Base: 1.5x per maturity level improvement (industry average)
    // Photon/Serverless SQL can provide 3-5x speedup
    let speedMultiplier = 1.0;
    if (analyticsPillar) {
      const analyticsImprovement = analyticsPillar.gap || 0;
      speedMultiplier += analyticsImprovement * 0.6; // +60% per level
      
      // Bonus if deploying Photon/Serverless
      const hasPhoton = analyticsPillar.databricksFeatures?.some(f => 
        f.name && (f.name.includes('Photon') || f.name.includes('Serverless'))
      );
      if (hasPhoton) {
        speedMultiplier += 0.8; // +80% boost from Photon
      }
    }
    
    if (dataEngPillar) {
      const dataImprovement = dataEngPillar.gap || 0;
      speedMultiplier += dataImprovement * 0.4; // +40% per level
      
      // Bonus for DLT automation
      const hasDLT = dataEngPillar.databricksFeatures?.some(f => 
        f.name && f.name.includes('Delta Live Tables')
      );
      if (hasDLT) {
        speedMultiplier += 0.5; // +50% boost from DLT
      }
    }
    
    // Apply industry multiplier and cap at reasonable maximum
    speedMultiplier = Math.min(speedMultiplier * multipliers.speed, 5.0);
    const decisionSpeed = speedMultiplier.toFixed(1) + '√ó';
    
    // 2. COST OPTIMIZATION (based on Platform Governance + automation level)
    const platformPillar = prioritizedActions.find(p => p.pillarId === 'platform_governance');
    const opExPillar = prioritizedActions.find(p => p.pillarId === 'operational_excellence');
    
    // Base: 3-5% per maturity level
    let costSavings = 0;
    if (platformPillar) {
      const platformImprovement = platformPillar.gap || 0;
      costSavings += platformImprovement * 4; // 4% per level
      
      // Bonus for serverless/auto-scaling
      const hasServerless = platformPillar.databricksFeatures?.some(f => 
        f.name && (f.name.includes('Serverless') || f.name.includes('Auto Scaling'))
      );
      if (hasServerless) {
        costSavings += 8; // +8% from serverless
      }
      
      // Bonus for budget controls
      const hasBudgets = platformPillar.databricksFeatures?.some(f => 
        f.name && (f.name.includes('Budget') || f.name.includes('Cost'))
      );
      if (hasBudgets) {
        costSavings += 5; // +5% from budget controls
      }
    }
    
    if (opExPillar) {
      const opExImprovement = opExPillar.gap || 0;
      costSavings += opExImprovement * 3; // 3% per level
    }
    
    // Apply industry multiplier and cap at 40%
    costSavings = Math.min(costSavings * multipliers.cost, 40);
    const costOptimization = Math.round(costSavings) + '%';
    
    // 3. OPERATIONAL OVERHEAD REDUCTION (based on automation maturity)
    let overheadReduction = 0;
    
    // Data Engineering automation
    if (dataEngPillar) {
      const dataImprovement = dataEngPillar.gap || 0;
      overheadReduction += dataImprovement * 12; // 12% per level
      
      // Bonus for automation features
      const hasAutomation = dataEngPillar.databricksFeatures?.some(f => 
        f.name && (f.name.includes('Auto Loader') || f.name.includes('DLT') || f.name.includes('Workflow'))
      );
      if (hasAutomation) {
        overheadReduction += 15; // +15% from automation
      }
    }
    
    // ML automation
    const mlPillar = prioritizedActions.find(p => p.pillarId === 'machine_learning');
    if (mlPillar) {
      const mlImprovement = mlPillar.gap || 0;
      overheadReduction += mlImprovement * 8; // 8% per level
      
      // Bonus for MLOps automation
      const hasMLOps = mlPillar.databricksFeatures?.some(f => 
        f.name && (f.name.includes('MLflow') || f.name.includes('Model Serving'))
      );
      if (hasMLOps) {
        overheadReduction += 10; // +10% from MLOps
      }
    }
    
    // Governance automation
    if (platformPillar) {
      const platformImprovement = platformPillar.gap || 0;
      overheadReduction += platformImprovement * 6; // 6% per level
      
      // Bonus for Unity Catalog (reduces manual access management)
      const hasUnityCatalog = platformPillar.databricksFeatures?.some(f => 
        f.name && f.name.includes('Unity Catalog')
      );
      if (hasUnityCatalog) {
        overheadReduction += 12; // +12% from Unity Catalog
      }
    }
    
    // Apply industry multiplier and cap at 65%
    overheadReduction = Math.min(overheadReduction * multipliers.overhead, 65);
    const manualOverhead = Math.round(overheadReduction) + '%';
    
    // 4. TIME TO MARKET / INNOVATION VELOCITY (based on ML + Gen AI maturity)
    const genAIPillar = prioritizedActions.find(p => p.pillarId === 'generative_ai');
    
    let timeToMarket = 0;
    if (mlPillar) {
      const mlImprovement = mlPillar.gap || 0;
      timeToMarket += mlImprovement * 15; // 15% per level
      
      // Bonus for Feature Store (speeds up model development)
      const hasFeatureStore = mlPillar.databricksFeatures?.some(f => 
        f.name && f.name.includes('Feature Store')
      );
      if (hasFeatureStore) {
        timeToMarket += 20; // +20% from Feature Store
      }
    }
    
    if (genAIPillar) {
      const genAIImprovement = genAIPillar.gap || 0;
      timeToMarket += genAIImprovement * 12; // 12% per level
      
      // Bonus for pre-built LLMs
      const hasLLMs = genAIPillar.databricksFeatures?.some(f => 
        f.name && (f.name.includes('DBRX') || f.name.includes('Foundation Model'))
      );
      if (hasLLMs) {
        timeToMarket += 25; // +25% from foundation models
      }
    }
    
    // Cap at 70%
    timeToMarket = Math.min(timeToMarket, 70);
    const timeToMarketValue = Math.round(timeToMarket) + '%';
    
    // 5. DATA QUALITY & TRUST (based on Data Engineering + Platform Governance)
    let dataQuality = 0;
    if (dataEngPillar) {
      const dataImprovement = dataEngPillar.gap || 0;
      dataQuality += dataImprovement * 10; // 10% per level
      
      // Bonus for Delta Lake (ACID compliance)
      const hasDelta = dataEngPillar.databricksFeatures?.some(f => 
        f.name && f.name.includes('Delta')
      );
      if (hasDelta) {
        dataQuality += 15; // +15% from Delta Lake
      }
      
      // Bonus for Data Quality features
      const hasDQ = dataEngPillar.databricksFeatures?.some(f => 
        f.name && (f.name.includes('Quality') || f.name.includes('Expectations'))
      );
      if (hasDQ) {
        dataQuality += 12; // +12% from DQ tools
      }
    }
    
    if (platformPillar) {
      const platformImprovement = platformPillar.gap || 0;
      dataQuality += platformImprovement * 8; // 8% per level
      
      // Bonus for Unity Catalog (lineage + governance)
      const hasUnityCatalog = platformPillar.databricksFeatures?.some(f => 
        f.name && f.name.includes('Unity Catalog')
      );
      if (hasUnityCatalog) {
        dataQuality += 18; // +18% from Unity Catalog
      }
    }
    
    // Cap at 60%
    dataQuality = Math.min(dataQuality, 60);
    const dataQualityValue = Math.round(dataQuality) + '%';
    
    // 6. TEAM PRODUCTIVITY / DEVELOPER EFFICIENCY (based on all pillars)
    let productivity = 0;
    
    // Calculate weighted average improvement
    prioritizedActions.forEach(pillar => {
      const improvement = pillar.gap || 0;
      productivity += improvement * 8; // 8% per level per pillar
    });
    
    // Bonus for collaborative features
    if (analyticsPillar) {
      const hasNotebooks = analyticsPillar.databricksFeatures?.some(f => 
        f.name && f.name.includes('Notebook')
      );
      if (hasNotebooks) {
        productivity += 12; // +12% from collaborative notebooks
      }
    }
    
    // Bonus for automation
    if (opExPillar) {
      const hasWorkflows = opExPillar.databricksFeatures?.some(f => 
        f.name && f.name.includes('Workflow')
      );
      if (hasWorkflows) {
        productivity += 15; // +15% from workflow automation
      }
    }
    
    // Cap at 55%
    productivity = Math.min(productivity, 55);
    const productivityValue = Math.round(productivity) + '%';
    
    const impact = {
      decisionSpeed: {
        value: decisionSpeed,
        label: 'Increase in analytics-driven decision-making speed',
        drivers: [
          analyticsPillar ? `${analyticsPillar.pillarName} improvement (${analyticsPillar.gap} levels)` : null,
          dataEngPillar ? `${dataEngPillar.pillarName} automation` : null,
          'Real-time analytics and query acceleration'
        ].filter(Boolean).slice(0, 2)
      },
      costOptimization: {
        value: costOptimization,
        label: 'Average cost optimization through platform automation',
        drivers: [
          platformPillar ? `${platformPillar.pillarName} improvements` : null,
          'Serverless compute and auto-scaling',
          'Budget controls and monitoring'
        ].filter(Boolean).slice(0, 2)
      },
      manualOverhead: {
        value: manualOverhead,
        label: 'Reduction in manual operational overhead',
        drivers: [
          dataEngPillar ? 'Data pipeline automation' : null,
          mlPillar ? 'MLOps automation' : null,
          platformPillar ? 'Governance automation (Unity Catalog)' : null
        ].filter(Boolean).slice(0, 2)
      },
      timeToMarket: {
        value: timeToMarketValue,
        label: 'Faster time-to-market for AI/ML initiatives',
        drivers: [
          mlPillar ? `${mlPillar.pillarName} acceleration` : null,
          genAIPillar ? `${genAIPillar.pillarName} with foundation models` : null,
          'Pre-trained models and feature engineering'
        ].filter(Boolean).slice(0, 2)
      },
      dataQuality: {
        value: dataQualityValue,
        label: 'Improvement in data quality and stakeholder trust',
        drivers: [
          dataEngPillar ? 'Delta Lake ACID compliance' : null,
          platformPillar ? 'Unity Catalog governance & lineage' : null,
          'Automated data quality monitoring'
        ].filter(Boolean).slice(0, 2)
      },
      teamProductivity: {
        value: productivityValue,
        label: 'Increase in data & engineering team productivity',
        drivers: [
          'Collaborative workspace and notebooks',
          opExPillar ? 'Workflow automation and orchestration' : null,
          'Reduced context switching and tool fragmentation'
        ].filter(Boolean).slice(0, 2)
      }
    };
    
    console.log('[calculateBusinessImpact] Calculated 6 impact metrics:', JSON.stringify(impact, null, 2));
    return impact;
  }
  
  /**
   * Generate dynamic maturity summary based on assessment data
   */
  generateMaturitySummary(assessment, prioritizedActions, currentScore, targetScore, industry = 'Technology') {
    console.log('[generateMaturitySummary] Generating for:', { currentScore, targetScore, industry });
    
    // Sort pillars by maturity (to identify strengths and weaknesses)
    const sorted = [...prioritizedActions].sort((a, b) => b.currentScore - a.currentScore);
    
    const strongPillars = sorted.filter(p => p.currentScore >= 4).slice(0, 2);
    const weakPillars = sorted.filter(p => p.currentScore <= 2).slice(0, 2);
    const criticalGaps = sorted.filter(p => p.gap >= 2).slice(0, 2);
    
    // Current Maturity Description - Executive-friendly with business context
    let currentDescription = '';
    if (strongPillars.length > 0) {
      const strongNames = strongPillars.map(p => p.pillarName.replace(/^[üèõÔ∏èüß±üíæüìäü§ñ‚ú®‚öôÔ∏è]+\s*/, '')).join(' and ');
      
      // Map pillar to business capability (not technical feature)
      const businessCapabilities = {
        'platform_governance': 'enterprise governance framework',
        'data_engineering': 'data pipeline automation',
        'analytics_bi': 'analytics infrastructure',
        'machine_learning': 'ML operations',
        'generative_ai': 'AI capabilities',
        'operational_excellence': 'platform optimization'
      };
      
      const primaryPillar = strongPillars[0];
      const capability = businessCapabilities[primaryPillar.pillarId] || 'operational capabilities';
      currentDescription += `Strong ${strongNames} with ${capability} established`;
    }
    
    if (weakPillars.length > 0) {
      const weakNames = weakPillars.map(p => p.pillarName.replace(/^[üèõÔ∏èüß±üíæüìäü§ñ‚ú®‚öôÔ∏è]+\s*/, '')).join(' and ');
      if (currentDescription) {
        currentDescription += ', with opportunity to accelerate ';
      } else {
        currentDescription += 'Positioned to accelerate ';
      }
      currentDescription += `${weakNames} ${weakPillars.length === 1 ? 'capability' : 'capabilities'} through automation and modernization`;
    }
    
    if (!currentDescription) {
      currentDescription = 'Balanced capability maturity with clear opportunities to drive operational efficiency and innovation velocity';
    }
    
    // Add business value context
    const businessValuePhrases = {
      'platform_governance': 'reducing risk and accelerating compliance',
      'data_engineering': 'improving data quality and reducing time-to-insight',
      'analytics_bi': 'enabling faster decision-making and business agility',
      'machine_learning': 'accelerating model development and improving model ROI',
      'generative_ai': 'unlocking new revenue streams and productivity gains',
      'operational_excellence': 'reducing operational overhead and improving team efficiency'
    };
    
    if (weakPillars.length > 0) {
      const primaryWeak = weakPillars[0];
      const businessValue = businessValuePhrases[primaryWeak.pillarId] || 'driving business transformation';
      currentDescription += `, positioning for strategic investment in ${businessValue}`;
    }
    
    // Add industry-specific business context
    const industryROI = {
      'Financial Services': 'to achieve competitive differentiation and regulatory efficiency',
      'Healthcare': 'to improve patient outcomes while reducing operational costs',
      'Retail': 'to increase revenue through personalized experiences and supply chain efficiency',
      'Manufacturing': 'to reduce costs and improve quality through predictive analytics',
      'Technology': 'to accelerate innovation and reduce time-to-market',
      'Telecommunications': 'to optimize network performance and increase customer satisfaction',
      'Energy': 'to improve operational efficiency and reduce compliance costs'
    };
    
    if (industry && industryROI[industry] && !currentDescription.includes('to achieve') && !currentDescription.includes('to improve')) {
      currentDescription += ` ${industryROI[industry]}`;
    }
    
    // Target Maturity Description - Business value framed
    let targetDescription = '';
    const targetBusinessOutcomes = {
      'platform_governance': 'enterprise-grade compliance and security posture',
      'data_engineering': 'real-time data pipelines driving faster insights',
      'analytics_bi': 'self-service analytics accelerating decision velocity',
      'machine_learning': 'production ML systems generating measurable ROI',
      'generative_ai': 'GenAI applications creating competitive differentiation',
      'operational_excellence': 'scalable operations reducing TCO'
    };
    
    if (criticalGaps.length > 0) {
      const improvements = criticalGaps.map(gap => {
        const pillarName = gap.pillarName.replace(/^[üèõÔ∏èüß±üíæüìäü§ñ‚ú®‚öôÔ∏è]+\s*/, '');
        const businessOutcome = targetBusinessOutcomes[gap.pillarId] || 'operational excellence';
        // Focus on business outcome, not technical feature name
        return `${pillarName} ${businessOutcome}`;
      });
      targetDescription = `Advanced capabilities in ${improvements.join(' and ')}`;
    }
    
    // Add governance if Platform is in focus
    const platformPillar = prioritizedActions.find(p => p.pillarId === 'platform_governance');
    if (platformPillar && platformPillar.gap >= 1) {
      const hasGovernance = targetDescription.toLowerCase().includes('platform');
      if (!hasGovernance) {
        targetDescription += targetDescription ? ', with unified governance reducing compliance risk and operational costs' : 'Unified governance and security reducing compliance risk and operational costs';
      }
    }
    
    if (!targetDescription) {
      targetDescription = 'Optimized, governed, and automated capabilities driving measurable business outcomes, cost savings, and competitive advantage across all pillars';
    }
    
    // Add business value ROI statement
    if (!targetDescription.includes('ROI') && !targetDescription.includes('revenue')) {
      targetDescription += ', maximizing ROI through operational efficiency';
    }
    
    // Improvement Potential Description - Business capability focus
    let improvementDescription = '';
    const topGaps = sorted.filter(p => p.gap >= 1).slice(0, 3);
    
    if (topGaps.length > 0) {
      const capabilities = topGaps.map(gap => {
        const pillarName = gap.pillarName.replace(/^[üèõÔ∏èüß±üíæüìäü§ñ‚ú®‚öôÔ∏è]+\s*/, '');
        // Map to business capability improvements
        const improvementAreas = {
          'platform_governance': 'governance and compliance automation',
          'data_engineering': 'data pipeline automation and quality controls',
          'analytics_bi': 'self-service analytics and real-time insights',
          'machine_learning': 'ML automation and lifecycle management',
          'generative_ai': 'GenAI applications with enterprise controls',
          'operational_excellence': 'operational automation and best practices'
        };
        return improvementAreas[gap.pillarId] || `${pillarName.toLowerCase()} automation`;
      });
      improvementDescription = `Implement ${capabilities.join(', ')} to achieve target maturity, reduce operational costs, and accelerate business transformation`;
    } else {
      improvementDescription = 'Optimize existing capabilities and expand automation coverage to drive continuous improvement and cost efficiency';
    }
    
    const maturitySummary = {
      current: {
        score: currentScore,
        level: this.getMaturityLevelName(currentScore),
        description: currentDescription
      },
      target: {
        score: targetScore,
        level: this.getMaturityLevelName(targetScore),
        description: targetDescription
      },
      improvement: {
        gap: targetScore - currentScore,
        timeline: '6‚Äì12 months',
        description: improvementDescription
      },
      roadmapIntro: this.generateRoadmapIntro(prioritizedActions, industry)
    };
    
    console.log('[generateMaturitySummary] Generated summary:', JSON.stringify(maturitySummary, null, 2));
    return maturitySummary;
  }
  
  getMaturityLevelName(score) {
    const roundedScore = Math.round(score);
    const levels = {
      1: 'Explore',
      2: 'Experiment',
      3: 'Formalize',
      4: 'Optimize',
      5: 'Transform'
    };
    return levels[roundedScore] || 'Formalize';
  }
  
  generateRoadmapIntro(prioritizedActions, industry) {
    const criticalGaps = prioritizedActions.filter(p => p.gap >= 2);
    const highPriority = prioritizedActions.filter(p => p.gap === 1);
    
    // Strategic transformation framing
    const industryContext = {
      'Financial Services': 'competitive differentiation in a rapidly evolving market',
      'Healthcare': 'improved patient outcomes and operational efficiency',
      'Retail': 'personalized customer experiences and supply chain optimization',
      'Manufacturing': 'operational excellence and predictive maintenance',
      'Technology': 'faster innovation cycles and product differentiation',
      'Telecommunications': 'network optimization and customer experience improvement',
      'Energy': 'operational efficiency and regulatory compliance'
    };
    
    let intro = 'This strategic transformation roadmap ';
    
    // Mention critical gaps with business framing
    if (criticalGaps.length > 0) {
      const gapNames = criticalGaps.map(p => {
        const name = p.pillarName.replace(/^[üèõÔ∏èüß±üíæüìäü§ñ‚ú®‚öôÔ∏è]+\s*/, '');
        return `${name} (${p.gap} ${p.gap === 1 ? 'level' : 'levels'})`;
      });
      intro += `accelerates your capabilities in ${gapNames.join(' and ')}, `;
    } else if (highPriority.length > 0) {
      const priorityNames = highPriority.slice(0, 2).map(p => 
        p.pillarName.replace(/^[üèõÔ∏èüß±üíæüìäü§ñ‚ú®‚öôÔ∏è]+\s*/, '')
      );
      intro += `drives strategic improvements in ${priorityNames.join(' and ')}, `;
    } else {
      intro += 'optimizes your existing capabilities, ';
    }
    
    // Mention focus areas with business outcomes
    const hasGovernance = criticalGaps.some(p => p.pillarId === 'platform_governance');
    const hasAutomation = criticalGaps.some(p => p.pillarId === 'data_engineering');
    const hasML = criticalGaps.some(p => p.pillarId === 'machine_learning');
    const hasGenAI = criticalGaps.some(p => p.pillarId === 'generative_ai');
    
    const focuses = [];
    if (hasGovernance) focuses.push('governance and risk management');
    if (hasAutomation) focuses.push('data pipeline automation');
    if (hasML) focuses.push('ML-driven decision making');
    if (hasGenAI) focuses.push('AI-powered innovation');
    
    if (focuses.length > 0) {
      intro += `prioritizing ${focuses.join(' and ')}`;
    } else {
      intro += 'enabling continuous improvement and competitive advantage';
    }
    
    // Add industry context with business outcome
    const industryOutcomes = {
      'Financial Services': ' to achieve competitive differentiation through reduced risk and accelerated compliance',
      'Healthcare': ' to improve patient outcomes while maintaining strict privacy and security standards',
      'Retail': ' to drive revenue growth through enhanced customer personalization and operational efficiency',
      'Manufacturing': ' to reduce operational costs and improve product quality through predictive analytics',
      'Technology': ' to accelerate time-to-market and enhance developer productivity',
      'Telecommunications': ' to optimize network performance and improve customer satisfaction',
      'Energy': ' to ensure operational safety and regulatory compliance while reducing operational costs'
    };
    
    if (industry && industryOutcomes[industry]) {
      intro += industryOutcomes[industry] + '.';
    } else {
      intro += ' to drive measurable business value and competitive advantage.';
    }
    
    return intro;
  }
  
  // ... (rest of the methods: extractPainPoints, extractComments, etc. - keeping them as-is)
  
  extractPainPoints(responses, framework) {
    const painPoints = [];
    if (!framework || !framework.dimensions) {
      console.log('[IntelligentEngine] No framework or dimensions');
      return painPoints;
    }
    
    console.log(`[IntelligentEngine] Extracting pain points from ${framework.dimensions.length} dimensions`);
    console.log('[IntelligentEngine] Sample response keys:', Object.keys(responses).slice(0, 5));
    
    framework.dimensions.forEach(dim => {
      dim.questions.forEach(q => {
        const techPain = q.perspectives?.find(p => p.id === 'technical_pain');
        if (techPain) {
          const responseKey = `${q.id}_technical_pain`;
          const selected = responses[responseKey];
          console.log(`[IntelligentEngine] Question ${q.id}, technical_pain response:`, selected);
          
          if (Array.isArray(selected) && selected.length > 0) {
            selected.forEach(value => {
              const option = techPain.options.find(o => o.value === value);
              if (option) {
                painPoints.push({ 
                  value, 
                  label: option.label, 
                  type: 'technical',
                  score: option.score || 3
                });
                console.log(`[IntelligentEngine] Found technical pain: ${option.label}`);
              }
            });
          }
        }
        
        const bizPain = q.perspectives?.find(p => p.id === 'business_pain');
        if (bizPain) {
          const responseKey = `${q.id}_business_pain`;
          const selected = responses[responseKey];
          
          if (Array.isArray(selected) && selected.length > 0) {
            selected.forEach(value => {
              const option = bizPain.options.find(o => o.value === value);
              if (option) {
                painPoints.push({ 
                  value, 
                  label: option.label, 
                  type: 'business',
                  score: option.score || 3
                });
                console.log(`[IntelligentEngine] Found business pain: ${option.label}`);
              }
            });
          }
        }
      });
    });
    
    console.log(`[IntelligentEngine] Total pain points extracted: ${painPoints.length}`);
    return painPoints;
  }

  extractComments(responses, framework) {
    const comments = [];
    if (!framework || !framework.dimensions) return comments;
    
    framework.dimensions.forEach(dim => {
      dim.questions.forEach(q => {
        const comment = responses[`${q.id}_comment`];
        if (comment && comment.trim()) {
          comments.push({ question: q.question, text: comment });
        }
      });
    });
    
    return comments;
  }

  analyzeStateGaps(responses, framework) {
    const gaps = [];
    if (!framework || !framework.dimensions) return gaps;
    
    framework.dimensions.forEach(dim => {
      dim.questions.forEach(q => {
        const current = responses[`${q.id}_current_state`];
        const future = responses[`${q.id}_future_state`];
        
        if (current !== undefined && future !== undefined) {
          gaps.push({
            question: q.question,
            dimension: dim.name,  // Add dimension name
            current: current,
            future: future,
            gap: future - current
          });
        }
      });
    });
    
    return gaps.sort((a, b) => b.gap - a.gap);
  }

  /**
   * üéØ Build context-aware "What's Working" based on assessment data
   * Analyzes current state, comments, and selected pain points
   */
  buildContextualStrengths(assessment, pillarId, painPoints, comments, stateGaps, pillarFramework) {
    const strengths = [];
    
    // Calculate average current score
    const avgCurrent = stateGaps.length > 0 ? stateGaps.reduce((sum, g) => sum + g.current, 0) / stateGaps.length : 0;
    
    // Extract mentions from comments
    const allComments = comments.map(c => c.text).join(' ').toLowerCase();
    
    // Pillar-specific strength strategies
    const pillarStrengths = {
      platform_governance: {
        basic: [
          allComments.includes('unity catalog') ? 'Unity Catalog deployed with centralized governance' : null,
          allComments.includes('rbac') || allComments.includes('role-based') ? 'Role-based access control implemented' : null,
          allComments.includes('sso') || allComments.includes('single sign') ? 'Single Sign-On (SSO) for unified authentication' : null,
          (avgCurrent >= 2) ? 'Environment separation established (dev/staging/prod)' : null,
          !painPoints.some(p => p.value.includes('audit')) ? 'Audit logging and compliance tracking active' : null
        ],
        advanced: [
          allComments.includes('abac') || allComments.includes('attribute-based') ? 'Attribute-Based Access Control (ABAC) for dynamic permissions' : null,
          allComments.includes('finops') || allComments.includes('cost attribution') ? 'FinOps practices with cost attribution and chargeback' : null,
          allComments.includes('automation') ? 'Automated provisioning and infrastructure-as-code' : null
        ]
      },
      data_engineering: {
        basic: [
          allComments.includes('delta') ? 'Delta Lake architecture for reliable data storage' : null,
          allComments.includes('dlt') || allComments.includes('delta live tables') ? 'Delta Live Tables for declarative ETL' : null,
          allComments.includes('streaming') ? 'Real-time streaming ingestion operational' : null,
          (avgCurrent >= 2) ? 'Medallion architecture (Bronze/Silver/Gold) in place' : null,
          !painPoints.some(p => p.value.includes('quality')) ? 'Data quality validation processes established' : null
        ],
        advanced: [
          allComments.includes('expectations') ? 'DLT Expectations for automated quality checks' : null,
          allComments.includes('auto loader') ? 'Auto Loader for incremental ingestion' : null,
          allComments.includes('orchestration') ? 'Automated pipeline orchestration with dependencies' : null
        ]
      },
      analytics_bi: {
        basic: [
          allComments.includes('dashboard') ? 'Self-service dashboards operational' : null,
          allComments.includes('sql warehouse') || allComments.includes('serverless') ? 'Serverless SQL warehouses for elastic compute' : null,
          (avgCurrent >= 2) ? 'Governed data access with certified datasets' : null,
          !painPoints.some(p => p.value.includes('performance')) ? 'Query performance optimization in place' : null,
          allComments.includes('tableau') || allComments.includes('power bi') ? 'Enterprise BI tool integration established' : null
        ],
        advanced: [
          allComments.includes('delta sharing') ? 'Delta Sharing for secure data distribution' : null,
          allComments.includes('genie') ? 'Genie AI Analyst for natural language queries' : null,
          allComments.includes('federation') ? 'Query Federation for cross-platform analytics' : null
        ]
      },
      machine_learning: {
        basic: [
          allComments.includes('mlflow') ? 'MLflow for experiment tracking and model registry' : null,
          allComments.includes('model serving') ? 'Model Serving endpoints for production inference' : null,
          (avgCurrent >= 2) ? 'Structured ML lifecycle with versioning' : null,
          !painPoints.some(p => p.value.includes('deployment')) ? 'Established model deployment processes' : null,
          allComments.includes('feature store') ? 'Feature Store for centralized feature management' : null
        ],
        advanced: [
          allComments.includes('automl') ? 'AutoML for accelerated model development' : null,
          allComments.includes('monitoring') ? 'Model monitoring for drift detection' : null,
          allComments.includes('distributed training') ? 'Distributed training for large-scale models' : null
        ]
      },
      generative_ai: {
        basic: [
          allComments.includes('vector search') ? 'Vector Search deployed for RAG applications' : null,
          allComments.includes('ai gateway') ? 'AI Gateway for governed LLM access' : null,
          (avgCurrent >= 2) ? 'GenAI experimentation framework established' : null,
          allComments.includes('prompt') ? 'Prompt engineering practices in place' : null,
          !painPoints.some(p => p.value.includes('safety')) ? 'LLM safety and guardrails implemented' : null
        ],
        advanced: [
          allComments.includes('fine-tuning') || allComments.includes('fine tuning') ? 'Custom LLM fine-tuning capabilities' : null,
          allComments.includes('multi-agent') ? 'Multi-agent systems for complex workflows' : null,
          allComments.includes('evaluation') ? 'LLM evaluation framework operational' : null
        ]
      },
      operational_excellence: {
        basic: [
          allComments.includes('system tables') ? 'System Tables for platform observability' : null,
          allComments.includes('monitoring') ? 'Proactive monitoring and alerting' : null,
          (avgCurrent >= 2) ? 'Cost tracking and budget management processes' : null,
          !painPoints.some(p => p.value.includes('incident')) ? 'Incident management framework in place' : null,
          allComments.includes('automation') ? 'Operational automation for common tasks' : null
        ],
        advanced: [
          allComments.includes('predictive') ? 'Predictive optimization for automatic maintenance' : null,
          allComments.includes('finops') ? 'FinOps practices with cost attribution' : null,
          allComments.includes('sla') ? 'SLA monitoring and enforcement' : null
        ]
      }
    };
    
    const pillarData = pillarStrengths[pillarId] || pillarStrengths.operational_excellence;
    
    // Add basic strengths
    strengths.push(...pillarData.basic.filter(s => s !== null));
    
    // Add advanced if maturity is high enough
    if (avgCurrent >= 3) {
      strengths.push(...pillarData.advanced.filter(s => s !== null));
    }
    
    // If still empty, add default strengths based on non-selected pain points
    if (strengths.length === 0) {
      const selectedPainValues = new Set(painPoints.map(p => p.value));
      const defaultStrengths = {
        platform_governance: [
          !selectedPainValues.has('auth_complexity') ? 'Basic authentication and access control' : null,
          !selectedPainValues.has('no_audit_logs') ? 'Audit logging capabilities available' : null,
          !selectedPainValues.has('poor_isolation') ? 'Environment isolation in place' : null
        ],
        data_engineering: [
          !selectedPainValues.has('manual_pipelines') ? 'Pipeline automation capabilities' : null,
          !selectedPainValues.has('slow_pipelines') ? 'Acceptable pipeline performance' : null,
          !selectedPainValues.has('data_quality_issues') ? 'Basic data quality processes' : null
        ],
        analytics_bi: [
          !selectedPainValues.has('slow_queries') ? 'Acceptable query performance' : null,
          !selectedPainValues.has('limited_adoption') ? 'Active analytics user base' : null,
          !selectedPainValues.has('access_barriers') ? 'Data accessibility for analysts' : null
        ],
        machine_learning: [
          !selectedPainValues.has('no_tracking') ? 'Experiment tracking capabilities' : null,
          !selectedPainValues.has('manual_deployment') ? 'Model deployment processes' : null,
          !selectedPainValues.has('no_monitoring') ? 'Basic model monitoring' : null
        ],
        generative_ai: [
          !selectedPainValues.has('no_strategy') ? 'GenAI exploration and experimentation' : null,
          !selectedPainValues.has('no_infrastructure') ? 'LLM infrastructure available' : null,
          !selectedPainValues.has('no_guardrails') ? 'Safety considerations in place' : null
        ],
        operational_excellence: [
          !selectedPainValues.has('no_monitoring') ? 'Platform monitoring capabilities' : null,
          !selectedPainValues.has('manual_response') ? 'Incident response procedures' : null,
          !selectedPainValues.has('high_costs') ? 'Cost management awareness' : null
        ]
      };
      
      const defaults = (defaultStrengths[pillarId] || defaultStrengths.operational_excellence).filter(s => s !== null);
      strengths.push(...defaults.slice(0, 3));
    }
    
    console.log(`[buildContextualStrengths] Generated ${strengths.length} strengths for ${pillarId}`);
    return strengths.slice(0, 5);
  }
  
  /**
   * üéØ Build context-aware "Key Challenges" based on pain points and assessment data
   */
  buildContextualChallenges(assessment, pillarId, topPainPoints, comments, stateGaps) {
    const challenges = [];
    
    // Extract meaningful challenges from pain points with context
    topPainPoints.slice(0, 5).forEach(pp => {
      // Enrich pain point labels with context if available
      const label = pp.label;
      
      // Find related comments for this pain point
      const relatedComment = comments.find(c => 
        c.text.toLowerCase().includes(pp.value.replace(/_/g, ' '))
      );
      
      if (relatedComment) {
        // Extract specific detail from comment
        const detail = relatedComment.text.substring(0, 100);
        challenges.push(`${label} ‚Äî currently ${detail}...`);
      } else {
        challenges.push(label);
      }
    });
    
    console.log(`[buildContextualChallenges] Generated ${challenges.length} challenges for ${pillarId}`);
    return challenges;
  }

  /**
   * DEPRECATED: Use buildContextualStrengths instead
   */
  extractStrengths(comments, painPoints, stateGaps, pillarId, pillarFramework) {
    const strengths = [];
    
    console.log(`[extractStrengths] Pillar: ${pillarId}, Pain points: ${painPoints.length}, State gaps: ${stateGaps.length}`);
    
    // Get all possible pain point values for THIS pillar from framework
    const allPillarPainPoints = this.getAllPainPointsFromFramework(pillarFramework);
    const selectedPainValues = new Set(painPoints.map(pp => pp.value));
    
    console.log(`[extractStrengths] All possible pain points in ${pillarId}:`, allPillarPainPoints.size);
    console.log(`[extractStrengths] Selected pain points:`, selectedPainValues.size);
    
    // Strategy 1: Extract SPECIFIC tool/capability mentions from comments (highest value)
    comments.forEach(c => {
      const specificMentions = this.extractSpecificCapabilities(c.text);
      if (specificMentions) {
        strengths.push(specificMentions);
      }
    });
    
    // Strategy 2: Identify what they're NOT struggling with (pillar-specific actual capabilities)
    const pillarSpecificStrengths = this.identifyAbsentPainPoints(
      allPillarPainPoints, 
      selectedPainValues, 
      pillarId
    );
    if (pillarSpecificStrengths.length > 0) {
      console.log(`[extractStrengths] Found ${pillarSpecificStrengths.length} pillar-specific strengths`);
      strengths.push(...pillarSpecificStrengths);
    }
    
    // Strategy 3: For high-scoring areas, extract dimension-specific practices
    const highScoreQuestions = stateGaps.filter(gap => gap.current >= 4);
    if (highScoreQuestions.length > 0 && strengths.length < 5) {
      console.log(`[extractStrengths] Found ${highScoreQuestions.length} high-score dimensions`);
      // Add dimension names as context (not score-based statements)
      highScoreQuestions.slice(0, 2).forEach(q => {
        const practiceStrength = this.describePractice(q.dimension, pillarId, true);
        if (practiceStrength) {
          strengths.push(practiceStrength);
        }
      });
    }
    
    // Deduplicate and limit
    const uniqueStrengths = [...new Set(strengths)].slice(0, 5);
    console.log(`[extractStrengths] Final strengths for ${pillarId}:`, uniqueStrengths.length);
    return uniqueStrengths;
  }
  
  getAllPainPointsFromFramework(pillarFramework) {
    // Extract all pain point values that exist in this pillar's framework
    const allPainPoints = new Set();
    
    if (!pillarFramework || !pillarFramework.dimensions) {
      return allPainPoints;
    }
    
    pillarFramework.dimensions.forEach(dim => {
      dim.questions.forEach(q => {
        q.perspectives?.forEach(p => {
          if (p.id.includes('pain')) {
            p.options?.forEach(opt => {
              allPainPoints.add(opt.value);
            });
          }
        });
      });
    });
    
    return allPainPoints;
  }
  
  describePractice(dimensionName, pillarId, isHighMaturity) {
    // Map dimension names to actual practices/capabilities (NOT score statements)
    const practiceMap = {
      // Platform Governance
      'Environment Architecture & Scalability': isHighMaturity ? 'Multi-environment setup with automated provisioning and scaling' : 'Basic environment separation with manual provisioning',
      'Identity, Security & Access Control': isHighMaturity ? 'Role-based access control with SSO and fine-grained permissions' : 'Basic authentication with limited access controls',
      'Governance & Compliance': isHighMaturity ? 'Automated compliance monitoring with audit trails and policy enforcement' : 'Manual compliance tracking with periodic reviews',
      'Observability & Monitoring': isHighMaturity ? 'Real-time monitoring with alerting and automated incident response' : 'Basic logging with manual log review',
      
      // Data Engineering
      'Lakehouse Architecture': isHighMaturity ? 'Bronze/Silver/Gold medallion architecture with automated promotion' : 'Basic data lake structure with manual curation',
      'Ingestion Strategy': isHighMaturity ? 'Automated ingestion with schema evolution and quality validation' : 'Manual batch ingestion with limited validation',
      'Pipeline Orchestration': isHighMaturity ? 'Event-driven orchestration with dependency management and retries' : 'Scheduled jobs with basic error handling',
      'Data Quality & Validation': isHighMaturity ? 'Automated quality checks with quarantine and remediation workflows' : 'Ad-hoc quality validation with manual fixes',
      
      // Analytics & BI
      'Analytic Performance': isHighMaturity ? 'Query optimization with caching, indexing, and performance monitoring' : 'Basic queries with manual optimization',
      'Semantic Layer & Metrics': isHighMaturity ? 'Centralized metrics layer with governed definitions and lineage' : 'Ad-hoc metrics with inconsistent definitions',
      'BI & Reporting': isHighMaturity ? 'Self-service dashboards with scheduled delivery and alerts' : 'Manual report generation with limited self-service',
      'Self-Service Analytics': isHighMaturity ? 'Governed self-service with data discovery and assisted exploration' : 'Limited self-service with IT support required',
      
      // Machine Learning
      'ML Lifecycle Management': isHighMaturity ? 'End-to-end MLOps with automated training, testing, and deployment' : 'Manual ML workflows with limited tracking',
      'Experiment Tracking & Management': isHighMaturity ? 'Centralized experiment tracking with versioning and reproducibility' : 'Notebook-based tracking with manual documentation',
      'Model Deployment & Serving': isHighMaturity ? 'Automated deployment with A/B testing and canary releases' : 'Manual model deployment with basic serving',
      'Model Monitoring & Operations': isHighMaturity ? 'Drift detection with automated retraining and alerting' : 'Manual performance monitoring with periodic reviews',
      
      // GenAI
      'GenAI Strategy & Readiness': isHighMaturity ? 'Production GenAI applications with governance and quality controls' : 'Exploratory GenAI projects with limited governance',
      'Data & Knowledge Readiness': isHighMaturity ? 'Curated knowledge base with vector embeddings and retrieval optimization' : 'Basic document corpus with manual curation',
      'LLM & Foundation Model Strategy': isHighMaturity ? 'Multi-model strategy with fine-tuning and evaluation frameworks' : 'Basic LLM experimentation with off-the-shelf models',
      'RAG & Retrieval': isHighMaturity ? 'Production RAG pipeline with hybrid search and relevance tuning' : 'Basic RAG proof-of-concept with simple retrieval',
      
      // Operational Excellence
      'Center of Excellence (CoE)': isHighMaturity ? 'Active CoE with dedicated team, training programs, and best practice evangelism' : 'Informal platform support with ad-hoc guidance',
      'Platform Standards & Best Practices': isHighMaturity ? 'Documented standards with automated enforcement and compliance monitoring' : 'Emerging standards with manual review process',
      'Collaboration & Knowledge Sharing': isHighMaturity ? 'Active collaboration with shared repos, documentation, and community forums' : 'Basic collaboration via shared workspaces',
      'Cost Management & Optimization': isHighMaturity ? 'Proactive cost management with chargebacks, budgets, and optimization automation' : 'Cost tracking with manual review and optimization'
    };
    
    return practiceMap[dimensionName] || null;
  }
  
  identifyAbsentPainPoints(allPillarPainPoints, selectedValues, pillarId) {
    // Pain points mapped to strengths - ONLY if they exist in this pillar
    const strengthIndicators = {
      // Platform Governance
      'no_version_control': 'Git-based version control in use',
      'manual_provisioning': 'Automated environment provisioning',
      'no_iac': 'Infrastructure-as-code deployed',
      'poor_isolation': 'Strong environment isolation (dev/staging/prod)',
      'compliance_gaps': 'Compliance controls in place',
      'weak_access_control': 'Robust access control with RBAC',
      'no_audit_logs': 'Audit logging enabled',
      'manual_deployment': 'Automated deployment pipelines',
      
      // Data Engineering
      'manual_pipelines': 'Automated pipeline orchestration',
      'poor_quality': 'Data quality validation in place',
      'no_monitoring': 'Pipeline monitoring and alerting',
      'scattered_data': 'Organized lakehouse architecture',
      'no_documentation': 'Data pipelines documented',
      'pipeline_failures': 'Reliable pipeline execution',
      'error_handling': 'Robust error handling and retry logic',
      
      // Analytics & BI
      'slow_queries': 'Optimized query performance',
      'no_caching': 'Query result caching enabled',
      'limited_monitoring': 'Comprehensive query monitoring',
      'access_bottlenecks': 'Self-service analytics enabled',
      'inconsistent_performance': 'Consistent query performance',
      
      // Machine Learning
      'no_experiment_tracking': 'Experiment tracking system operational',
      'no_model_registry': 'Model registry in use',
      'scattered_artifacts': 'Centralized model artifact storage',
      'no_model_monitoring': 'Model monitoring in production',
      'manual_retraining': 'Automated model retraining',
      'no_feature_store': 'Feature store deployed',
      
      // GenAI
      'no_genai_strategy': 'GenAI strategy defined',
      'unclear_use_cases': 'Clear GenAI use cases identified',
      'no_vector_search': 'Vector search infrastructure',
      'prompt_management': 'Prompt versioning and management',
      'no_rag': 'RAG framework operational',
      
      // Operational Excellence
      'no_coe': 'Center of Excellence operational',
      'unclear_charter': 'Clear CoE charter and responsibilities',
      'resource_constraints': 'Adequate platform team resources',
      'standards_gaps': 'Platform standards documented',
      'no_training': 'Training program established',
      'poor_collaboration': 'Strong collaboration practices',
      'no_cost_tracking': 'Cost tracking and optimization'
    };
    
    const strengths = [];
    
    // Only check pain points that exist in THIS pillar
    for (const [painValue, strength] of Object.entries(strengthIndicators)) {
      if (allPillarPainPoints.has(painValue) && !selectedValues.has(painValue)) {
        strengths.push(strength);
      }
    }
    
    console.log(`[identifyAbsentPainPoints] ${pillarId}: Checking ${allPillarPainPoints.size} pillar pain points, found ${strengths.length} strengths`);
    
    return strengths;
  }
  
  extractSpecificCapabilities(commentText) {
    const text = commentText.toLowerCase();
    
    // Extract specific tool/capability mentions
    if (text.includes('unity catalog')) return 'Unity Catalog deployed for governance';
    if (text.includes('delta live tables') || text.includes('dlt')) return 'Delta Live Tables in production';
    if (text.includes('mlflow')) return 'MLflow for experiment tracking and model registry';
    if (text.includes('photon')) return 'Photon engine enabled for performance';
    if (text.includes('serverless')) return 'Serverless compute in use';
    if (text.includes('workflows')) return 'Databricks Workflows for orchestration';
    if (text.includes('vector search')) return 'Vector Search operational for RAG';
    if (text.includes('feature store')) return 'Feature Store for ML feature management';
    if (text.includes('lakehouse monitoring')) return 'Lakehouse Monitoring for data quality';
    
    return null;
  }
  
}

module.exports = IntelligentRecommendationEngine;

