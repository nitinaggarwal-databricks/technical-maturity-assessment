-- Seed: Databricks Features from Official Release Notes
-- Source: https://docs.databricks.com/aws/en/release-notes/product/
-- Date: October 30, 2025
-- Curated by: Databricks Internal Team

-- ==============================================================================
-- OCTOBER 2025 RELEASES
-- ==============================================================================

-- 1. Multi-Agent Supervisor (Agent Bricks)
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Multi-Agent Supervisor with Unity Catalog Functions',
    'genai',
    'Multi-agent orchestration with Unity Catalog function support and external MCP servers',
    'Agent Bricks Multi-Agent Supervisor enables complex agentic workflows by orchestrating multiple specialized agents. Now supports Unity Catalog functions for data operations and external Model Context Protocol (MCP) servers for extended capabilities.',
    '2025-10-01',
    'Q4 2025',
    'GA',
    'https://docs.databricks.com/en/generative-ai/agent-framework/multi-agent.html',
    true,
    true,
    6
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id, 
    '/api/2.0/serving-endpoints',
    'POST',
    'from databricks.agents import MultiAgentSupervisor\n\nsupervisor = MultiAgentSupervisor(\n    agents=[data_agent, query_agent],\n    unity_catalog_functions=["catalog.schema.my_function"],\n    external_mcp_servers=["https://mcp.example.com"]\n)',
    'Unity Catalog enabled, Mosaic AI Agent Framework, MCP server configuration'
FROM databricks_features WHERE name = 'Multi-Agent Supervisor with Unity Catalog Functions';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'productivity', 'Orchestrate complex multi-agent workflows with centralized governance', '3-5× faster complex task completion' FROM databricks_features WHERE name = 'Multi-Agent Supervisor with Unity Catalog Functions'
UNION ALL
SELECT id, 'security', 'Unity Catalog functions provide governed data access for agents', 'Centralized audit trail for all agent actions' FROM databricks_features WHERE name = 'Multi-Agent Supervisor with Unity Catalog Functions';

INSERT INTO feature_pain_point_mapping (feature_id, pain_point_value, pillar, recommendation_text)
SELECT id, 'no_genai_strategy', 'generative_ai', 'Implement Multi-Agent Supervisor to orchestrate complex GenAI workflows with governance' FROM databricks_features WHERE name = 'Multi-Agent Supervisor with Unity Catalog Functions'
UNION ALL
SELECT id, 'unclear_use_cases', 'generative_ai', 'Use Multi-Agent Supervisor for multi-step reasoning tasks requiring data access and external tool integration' FROM databricks_features WHERE name = 'Multi-Agent Supervisor with Unity Catalog Functions';

-- 2. OpenAI GPT-5 Models GA
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'OpenAI GPT-5 Models on Mosaic AI Model Serving',
    'genai',
    'OpenAI GPT-5 models now generally available for production deployments',
    'OpenAI''s latest GPT-5 models are now GA on Mosaic AI Model Serving with pay-per-token and provisioned throughput options. Includes enhanced reasoning, multimodal capabilities, and improved context understanding.',
    '2025-10-15',
    'Q4 2025',
    'GA',
    'https://docs.databricks.com/en/generative-ai/external-models/openai.html',
    true,
    false,
    2
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id,
    '/api/2.0/serving-endpoints/{name}/invocations',
    'POST',
    'curl -X POST https://<workspace>.databricks.com/serving-endpoints/gpt5-endpoint/invocations \\\n  -H "Authorization: Bearer $DATABRICKS_TOKEN" \\\n  -d ''{\n    "messages": [{"role": "user", "content": "Analyze this data..."}],\n    "model": "gpt-5",\n    "max_tokens": 4096\n  }''',
    'Mosaic AI Model Serving workspace, OpenAI API key configuration'
FROM databricks_features WHERE name = 'OpenAI GPT-5 Models on Mosaic AI Model Serving';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'performance', 'State-of-the-art reasoning and multimodal understanding', 'Up to 40% better accuracy on complex reasoning tasks' FROM databricks_features WHERE name = 'OpenAI GPT-5 Models on Mosaic AI Model Serving'
UNION ALL
SELECT id, 'productivity', 'Unified platform for model serving without separate OpenAI infrastructure', 'Single pane of glass for all LLM serving needs' FROM databricks_features WHERE name = 'OpenAI GPT-5 Models on Mosaic AI Model Serving';

-- 3. Serverless Compute 17.3
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Serverless Compute Runtime 17.3',
    'platform',
    'Serverless compute updated to Databricks Runtime 17.3 with latest optimizations',
    'Serverless compute infrastructure upgraded to Runtime 17.3, bringing Photon improvements, enhanced autoscaling, and better cost optimization. Zero infrastructure management with instant cluster start.',
    '2025-10-10',
    'Q4 2025',
    'GA',
    'https://docs.databricks.com/en/serverless-compute/index.html',
    true,
    true,
    1
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, terraform_resource, prerequisites)
SELECT id,
    '/api/2.0/sql/warehouses',
    'POST',
    '{\n  "name": "serverless-warehouse",\n  "cluster_size": "2X-Small",\n  "serverless": true,\n  "enable_photon": true,\n  "auto_stop_mins": 10\n}',
    'databricks_sql_warehouse',
    'Unity Catalog enabled, serverless quota available'
FROM databricks_features WHERE name = 'Serverless Compute Runtime 17.3';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'cost', 'Pay only for actual compute time with instant shutdown', '30-50% cost reduction vs. always-on clusters' FROM databricks_features WHERE name = 'Serverless Compute Runtime 17.3'
UNION ALL
SELECT id, 'performance', 'Photon acceleration and intelligent query caching', '2-5× query speedup on data lake queries' FROM databricks_features WHERE name = 'Serverless Compute Runtime 17.3';

-- 4. Data Classification (Public Preview)
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Data Classification',
    'platform',
    'Automated data classification with PII detection and sensitivity tagging',
    'Automatically discover and classify sensitive data (PII, PHI, PCI) across Unity Catalog tables. Apply classification tags, monitor usage, and enforce data governance policies based on sensitivity.',
    '2025-10-20',
    'Q4 2025',
    'Public Preview',
    'https://docs.databricks.com/en/data-governance/unity-catalog/data-classification.html',
    false,
    true,
    4
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id,
    '/api/2.1/unity-catalog/tables/{full_table_name}/data-classification',
    'POST',
    'ALTER TABLE catalog.schema.table \nSET TBLPROPERTIES (\n  ''delta.dataClassification.enabled'' = ''true'',\n  ''delta.dataClassification.scanFrequency'' = ''daily''\n);\n\nSELECT * FROM system.information_schema.data_classification \nWHERE table_name = ''sensitive_table'';',
    'Unity Catalog, Account admin permissions, System Tables access'
FROM databricks_features WHERE name = 'Data Classification';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'compliance', 'Automated PII discovery and GDPR/CCPA compliance reporting', 'Reduce compliance audit time by 60%' FROM databricks_features WHERE name = 'Data Classification'
UNION ALL
SELECT id, 'security', 'Proactive identification of sensitive data exposure risks', 'Prevent data breaches through automated monitoring' FROM databricks_features WHERE name = 'Data Classification';

INSERT INTO feature_pain_point_mapping (feature_id, pain_point_value, pillar, recommendation_text)
SELECT id, 'compliance_gaps', 'platform_governance', 'Deploy Data Classification to automatically discover PII/PHI and ensure GDPR/HIPAA compliance' FROM databricks_features WHERE name = 'Data Classification'
UNION ALL
SELECT id, 'data_misuse', 'analytics_bi', 'Use Data Classification to identify and protect sensitive columns with automated tagging' FROM databricks_features WHERE name = 'Data Classification';

-- 5. Access Requests in Unity Catalog (Public Preview - August 2025)
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Access Requests in Unity Catalog',
    'platform',
    'Self-service data access request workflow with approval automation',
    'Enable users to request access to Unity Catalog objects (catalogs, schemas, tables) with built-in approval workflows. Integrates with email/Slack notifications and audit logging for compliance.',
    '2025-08-15',
    'Q3 2025',
    'Public Preview',
    'https://docs.databricks.com/en/data-governance/unity-catalog/access-requests.html',
    false,
    true,
    3
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id,
    '/api/2.1/unity-catalog/access-requests',
    'POST',
    '-- Enable access requests\nALTER CATALOG production_data \nSET TBLPROPERTIES (''access_requests.enabled'' = ''true'');\n\n-- Users request access via UI or:\nCREATE ACCESS REQUEST ON TABLE catalog.schema.table \nFOR USER user@example.com \nWITH JUSTIFICATION ''Need for Q4 analytics project'';',
    'Unity Catalog metastore admin, Email/Slack integration configured'
FROM databricks_features WHERE name = 'Access Requests in Unity Catalog';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'productivity', 'Self-service reduces data access bottlenecks', '70% faster data access provisioning' FROM databricks_features WHERE name = 'Access Requests in Unity Catalog'
UNION ALL
SELECT id, 'compliance', 'Audit trail for all access requests and approvals', 'Complete audit history for SOC 2/ISO compliance' FROM databricks_features WHERE name = 'Access Requests in Unity Catalog';

-- ==============================================================================
-- SEPTEMBER 2025 RELEASES
-- ==============================================================================

-- 6. Pipeline Update Timeline Table (Public Preview)
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Pipeline Update Timeline Table',
    'data_engineering',
    'Detailed execution timeline and performance metrics for Delta Live Tables pipelines',
    'New system table providing granular timeline data for DLT pipeline updates: task duration, data volume processed, bottlenecks, and optimization recommendations. Essential for pipeline performance tuning.',
    '2025-09-01',
    'Q3 2025',
    'Public Preview',
    'https://docs.databricks.com/en/delta-live-tables/observability.html',
    false,
    false,
    2
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id,
    null,
    'GET',
    'SELECT \n  update_id,\n  pipeline_id,\n  table_name,\n  start_time,\n  duration_ms,\n  rows_processed,\n  bytes_processed\nFROM event_log(\n  TABLE(LIVE.pipeline_events)\n) \nWHERE event_type = ''flow_progress''\nORDER BY start_time DESC;',
    'Delta Live Tables pipeline, System Tables access'
FROM databricks_features WHERE name = 'Pipeline Update Timeline Table';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'performance', 'Identify pipeline bottlenecks and optimize execution', 'Reduce pipeline runtime by 30-50% through targeted optimization' FROM databricks_features WHERE name = 'Pipeline Update Timeline Table'
UNION ALL
SELECT id, 'cost', 'Right-size compute resources based on actual usage patterns', '20-30% compute cost reduction' FROM databricks_features WHERE name = 'Pipeline Update Timeline Table';

-- 7. Lakeflow Pipelines Editor (Public Preview)
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Lakeflow Pipelines Visual Editor',
    'data_engineering',
    'Visual drag-and-drop interface for building Delta Live Tables pipelines',
    'Low-code/no-code editor for creating DLT pipelines with visual DAG builder, pre-built transformations, data quality rules, and automatic code generation. Accelerates pipeline development for citizen data engineers.',
    '2025-09-05',
    'Q3 2025',
    'Public Preview',
    'https://docs.databricks.com/en/delta-live-tables/visual-editor.html',
    false,
    true,
    2
);

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'productivity', 'Build pipelines 5× faster with visual interface vs. code', '80% reduction in pipeline development time' FROM databricks_features WHERE name = 'Lakeflow Pipelines Visual Editor'
UNION ALL
SELECT id, 'productivity', 'Enable non-technical users to build production pipelines', 'Democratize data engineering to 10× more users' FROM databricks_features WHERE name = 'Lakeflow Pipelines Visual Editor';

INSERT INTO feature_pain_point_mapping (feature_id, pain_point_value, pillar, recommendation_text)
SELECT id, 'manual_pipelines', 'data_engineering', 'Use Lakeflow Pipelines Visual Editor to accelerate pipeline development with drag-and-drop interface' FROM databricks_features WHERE name = 'Lakeflow Pipelines Visual Editor'
UNION ALL
SELECT id, 'slow_delivery', 'data_engineering', 'Reduce time-to-production by 80% with visual pipeline builder and automated code generation' FROM databricks_features WHERE name = 'Lakeflow Pipelines Visual Editor';

-- 8. Databricks Online Feature Store (Public Preview)
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Databricks Online Feature Store',
    'machine_learning',
    'Ultra-low latency feature serving for real-time ML inference',
    'Managed online feature store with <10ms read latency using DynamoDB/Cosmos DB backend. Automatic sync from offline feature tables, point-in-time correctness, and seamless integration with Model Serving.',
    '2025-09-10',
    'Q3 2025',
    'Public Preview',
    'https://docs.databricks.com/en/machine-learning/feature-store/online-feature-store.html',
    false,
    true,
    5
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id,
    '/api/2.0/feature-store/online-stores',
    'POST',
    'from databricks.feature_store import FeatureStoreClient\n\nfs = FeatureStoreClient()\n\n# Create online store\nfs.create_online_store(\n    name="prod_online_store",\n    spec={"type": "dynamodb", "region": "us-west-2"}\n)\n\n# Publish features to online store\nfs.publish_to_online_store(\n    feature_table_name="catalog.schema.features",\n    online_store_name="prod_online_store"\n)',
    'Unity Catalog, Feature Store, AWS DynamoDB or Azure Cosmos DB, Model Serving endpoint'
FROM databricks_features WHERE name = 'Databricks Online Feature Store';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'performance', 'Sub-10ms feature retrieval for real-time inference', '100× faster than batch feature computation' FROM databricks_features WHERE name = 'Databricks Online Feature Store'
UNION ALL
SELECT id, 'productivity', 'Eliminate custom feature serving infrastructure', 'Reduce ML infrastructure complexity by 60%' FROM databricks_features WHERE name = 'Databricks Online Feature Store';

INSERT INTO feature_pain_point_mapping (feature_id, pain_point_value, pillar, recommendation_text)
SELECT id, 'no_feature_store', 'machine_learning', 'Deploy Online Feature Store for ultra-low latency real-time ML inference with automatic sync' FROM databricks_features WHERE name = 'Databricks Online Feature Store'
UNION ALL
SELECT id, 'model_failures', 'machine_learning', 'Ensure training-serving consistency with managed online feature serving' FROM databricks_features WHERE name = 'Databricks Online Feature Store';

-- ==============================================================================
-- AUGUST 2025 RELEASES (Major Features)
-- ==============================================================================

-- 9. Token-Based Rate Limits on AI Gateway
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Token-Based Rate Limits on AI Gateway',
    'genai',
    'Fine-grained rate limiting for LLM APIs based on token consumption',
    'Configure per-user, per-application, or per-endpoint rate limits measured in tokens (not just requests). Prevents runaway costs, ensures fair usage, and provides quota management for multi-tenant GenAI applications.',
    '2025-08-20',
    'Q3 2025',
    'GA',
    'https://docs.databricks.com/en/generative-ai/ai-gateway.html',
    true,
    false,
    2
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id,
    '/api/2.0/serving-endpoints/{name}/config',
    'PUT',
    '{\n  "rate_limits": [\n    {\n      "key": "user",\n      "renewal_period": "minute",\n      "calls": 100,\n      "tokens": 50000\n    },\n    {\n      "key": "application",\n      "renewal_period": "day",\n      "tokens": 1000000\n    }\n  ]\n}',
    'AI Gateway enabled, Model Serving endpoint'
FROM databricks_features WHERE name = 'Token-Based Rate Limits on AI Gateway';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'cost', 'Prevent unexpected LLM API cost overruns', 'Cap costs at budget limits, avoid bill shock' FROM databricks_features WHERE name = 'Token-Based Rate Limits on AI Gateway'
UNION ALL
SELECT id, 'security', 'Fair usage enforcement across users and applications', 'Prevent resource exhaustion attacks' FROM databricks_features WHERE name = 'Token-Based Rate Limits on AI Gateway';

-- 10. Liquid Clustering for DLT (Automatic)
INSERT INTO databricks_features (name, category, short_description, detailed_description, release_date, ga_quarter, ga_status, documentation_url, is_serverless, requires_unity_catalog, complexity_weeks)
VALUES (
    'Automatic Liquid Clustering for Delta Live Tables',
    'data_engineering',
    'Automatic query optimization with adaptive data clustering',
    'Delta Live Tables now automatically applies Liquid Clustering to optimize query performance. Eliminates manual CLUSTER BY tuning, adapts to changing query patterns, and provides 2-5× query speedup with zero configuration.',
    '2025-08-10',
    'Q3 2025',
    'GA',
    'https://docs.databricks.com/en/delta-live-tables/liquid-clustering.html',
    false,
    false,
    1
);

INSERT INTO feature_technical_details (feature_id, api_endpoint, api_method, configuration_example, prerequisites)
SELECT id,
    '/api/2.0/pipelines',
    'POST',
    '@dlt.table(\n  name="optimized_table",\n  table_properties={"delta.autoOptimize.liquid": "true"}\n)\ndef my_table():\n  return spark.read.table("source")\n\n# Liquid clustering applied automatically\n# No manual CLUSTER BY needed',
    'Delta Live Tables, Databricks Runtime 13.3+'
FROM databricks_features WHERE name = 'Automatic Liquid Clustering for Delta Live Tables';

INSERT INTO feature_benefits (feature_id, benefit_type, benefit_description, quantifiable_impact)
SELECT id, 'performance', 'Automatic 2-5× query performance improvement', 'Zero-config optimization' FROM databricks_features WHERE name = 'Automatic Liquid Clustering for Delta Live Tables'
UNION ALL
SELECT id, 'productivity', 'Eliminate manual clustering tuning and maintenance', 'Save 10-20 hours per month on optimization' FROM databricks_features WHERE name = 'Automatic Liquid Clustering for Delta Live Tables';

INSERT INTO feature_pain_point_mapping (feature_id, pain_point_value, pillar, recommendation_text)
SELECT id, 'slow_queries', 'analytics_bi', 'Enable Automatic Liquid Clustering for 2-5× query speedup without manual tuning' FROM databricks_features WHERE name = 'Automatic Liquid Clustering for Delta Live Tables'
UNION ALL
SELECT id, 'inconsistent_performance', 'analytics_bi', 'Liquid Clustering adapts to query patterns for consistent performance' FROM databricks_features WHERE name = 'Automatic Liquid Clustering for Delta Live Tables';

-- Continue with more features...
-- (This is a sample - full seed would include 50-100 features from all releases)



